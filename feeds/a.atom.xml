<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jerry Chi's website - a</title><link href="https://jerrychi.com/" rel="alternate"></link><link href="https://jerrychi.com/feeds/a.atom.xml" rel="self"></link><id>https://jerrychi.com/</id><updated>2022-08-25T00:00:00+09:00</updated><subtitle>Jerry Chi&lt;BR&gt;Data Scientist in Tokyo</subtitle><entry><title>How Stable Diffusion (Latent Diffusion) works</title><link href="https://jerrychi.com/how-stable-diffusion-latent-diffusion-works.html" rel="alternate"></link><published>2022-08-25T00:00:00+09:00</published><updated>2022-08-25T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2022-08-25:/how-stable-diffusion-latent-diffusion-works.html</id><summary type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;UPDATE 2022–10–07: a new, more beginner-friendly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;UPDATE 2022–10–07: a new, more beginner-friendly article on Stable Diffusion was posted at&lt;a href="https://jalammar.github.io/illustrated-stable-diffusion/"&gt;
https://jalammar.github.io/illustrated-stable-diffusion/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;But how does it actually work from a technical perspective?&lt;/p&gt;
&lt;p&gt;Basically, it uses a variational autoencoder (VAE) combined with a denoising diffusion model. The key idea is that using diffusion models in pixel space (the raw image) is not the most efficient approach, since there are many barely perceptible small details that are not efficiently learned by a diffusion model. Rather, we can use a VAE to map images into a latent space (a form of compression), and then train the diffusion model in the (much smaller) latent space of images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image_alt_text" src="https://miro.medium.com/max/640/0*iltP4V3p0cGAkL_O"&gt;
Source: &lt;a href="https://huggingface.co/blog/stable_diffusion"&gt;https://huggingface.co/blog/stable_diffusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stable Diffusion is &lt;a href="https://github.com/CompVis/stable-diffusion"&gt;basically a special case / specific configuration of Latent Diffusion&lt;/a&gt;. A lot of effort went into making it very high-quality and easy to use for the masses.&lt;/p&gt;
&lt;p&gt;The above explanation barely scratches the surface. &lt;a href="https://docs.google.com/document/d/1x4iHe9mdyqpuINRN2EYMuG6_0JSBoNnjtDdSte18Ugc/edit#"&gt;&lt;strong&gt;For more in-depth details on Stable Diffusion / Latent Diffusion, please see this google doc I made.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also, I compiled various AI art resources (both technical and non-technical) at &lt;a href="https://tinyurl.com/creative-ai-links"&gt;https://tinyurl.com/creative-ai-links&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, we need to wrap up with some actual AI art :) The below image was generated using Stable Diffusion at &lt;a href="https://beta.dreamstudio.ai/"&gt;https://beta.dreamstudio.ai/&lt;/a&gt; with the prompt “Character portrait of a graceful and pretty Korean princess with gorgeous detailed eyes and flowing hair, fantasy setting, color page, tankobon, 4k, tone mapping, doll, akihiko yoshida, james jean, andrei riabovitchev, marc simonetti, yoshitaka amano”
&lt;img alt="image_alt_text" src="https://miro.medium.com/max/720/1*ePNHU-hG80IQrfE8m7J-1w.jpeg"&gt;&lt;/p&gt;</content><category term="a"></category><category term="a"></category></entry></feed>