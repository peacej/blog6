<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jerry Chi's website - AI art</title><link href="https://jerrychi.com/" rel="alternate"></link><link href="https://jerrychi.com/feeds/ai-art.atom.xml" rel="self"></link><id>https://jerrychi.com/</id><updated>2022-09-26T00:00:00+09:00</updated><subtitle>Jerry Chi&lt;BR&gt;Data Scientist in Tokyo</subtitle><entry><title>My 2001 Stanford application essay on algorithmic music</title><link href="https://jerrychi.com/my-2001-stanford-application-essay-on-algorithmic-music.html" rel="alternate"></link><published>2022-09-26T00:00:00+09:00</published><updated>2022-09-26T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2022-09-26:/my-2001-stanford-application-essay-on-algorithmic-music.html</id><summary type="html">&lt;p&gt;In 2001, when I was in high school, I applied to Stanford University. 
I was truly excited about the topic of algorithmic music, and I tried hard to express that in my application. In the US, for some colleges, the essay can matter a lot. Perhaps that's one of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In 2001, when I was in high school, I applied to Stanford University. 
I was truly excited about the topic of algorithmic music, and I tried hard to express that in my application. In the US, for some colleges, the essay can matter a lot. Perhaps that's one of the reasons why I got into Stanford and not UC Berkeley. I was thinking hard about majoring in either Music or Computer Science (but ended up majoring in Management Science &amp;amp; Engineering, since I was interested in too many topics).&lt;/p&gt;
&lt;p&gt;It's funny that I was deeply inspired by computational creativity then, and now, 20 years later, I am obsessed with it again. (I think AI art is a subcategory of computational creativity, loosely speaking.)&lt;/p&gt;
&lt;p&gt;It's also amazing how far we've come technology-wise since 2001. I took Stanford's Intro to AI then as a high school student, in the middle of an AI winter, and my conclusion was basically "AI doesn't work."
The recent explosion in visual AI art is nothing short of spectacular, and, although less prominent on social media, people are using deep learning methods like GANs and VAEs (variational autoencoders) in interesting ways to generate good music as well (check out the &lt;a href="https://2022.aimusiccreativity.org/"&gt;videos/papers from the recent AI Music Creativity Conference&lt;/a&gt;, especially the &lt;a href="https://www.youtube.com/watch?v=3jvpPHg3WH0&amp;amp;list=PL0Qzdw4De5462yuVUMy2UU-N4fS-IdaZ3&amp;amp;index=2"&gt;keynote on performing music with AI by Nao Tokui&lt;/a&gt;).  The rate of improvement is amazing.&lt;/p&gt;
&lt;p&gt;Anyway...&lt;/p&gt;
&lt;p&gt;〜〜 below is the 2001 essay (warning: it is a bit corny/cringeworthy) 〜〜&lt;/p&gt;
&lt;p&gt;Six months ago, Serendipity hit me, picked me up, and threw me towards the sky at escape velocity.  It was in the form of Professor Diana Dabby (Olin College), who made an extraordinary musical presentation at my high school.   All the students filed into the gym and sat on rows of green chairs, awaiting boredom.  Still sweaty from basketball, I fanned myself with the book I was going to read during the assembly; I wanted to make good use of assembly time.  But the subsequent show would turn out to be worth a thousand books.&lt;/p&gt;
&lt;p&gt;First, the professor played a tape of an excerpt from J.S. Bach’s A Well-Tempered Clavier, a familiar Baroque piano piece.  She then played a strange, yet euphonic modification of the selection.  After its oddly bittersweet conclusion, Professor Dabby calmly revealed the genius behind the modified composition:  a computer.  A computer!?  “That’s right,” she affirmed, “I just used a computer to combine the song and some mathematical equations.”  While keeping the essence of Bach, the computer had brought something entirely new out of the piece, infusing an arbitrary aspect into it. Reminiscent of my past contemplation, the dreamlike idea of the integration of math and music was now right in front of me.  &lt;/p&gt;
&lt;p&gt;It got better, aurally.  After a few sweet minutes of Bach, the professor proceeded to play the original and modified versions of George Gershwin’s “Prelude No. 1,” a Contemporary piano piece that I had recently mastered.  Her choice of “my song” made me realize and appreciate the universality of my music education; like math, music was really a worldwide language.  The professor’s presentation immersed me in that language, urging me to harness its true power.  My hands clenched a few times, enjoying their newfound importance and potential.  Before this experience, improvising here and there, exhibiting a jazzy, rubato style, I had believed that my performance of the prelude had reached its musical limits.  But for me, listening to and swallowing the Gershwin “remix” – with its increased jazziness, chaos, and uniqueness – transcended any previous experience of the piece. &lt;/p&gt;
&lt;p&gt;After the prelude was over, my rapture was not; Professor Dabby started to explain the music’s mathematical mechanisms.  She reached into her briefcase, produced a transparency, and placed it on an overhead projector.  I saw a 3D graph, but it looked more like tangled thread than anything else.  Pointing at it, Dabby explained that the three coordinates of each point corresponded to the pitch, amplitude, and length of a note.  To put the song into the computer, the professor had mapped each note of the music to a point on the graph.  She could then change the coefficients in the equations to transpose the notes to new positions and thus create a different song.  After experimenting with the various modifications, Dabby could discriminate among them.  The result: a collection of hundreds of compositional changes no human could even think of, done with the click of a mouse and a little editing.  A masterpiece.   Although the method was a little recondite, I felt it was both nothing I couldn’t do and the beginning of what I could do.&lt;br&gt;
When her presentation was over, the professor lingered in the gym.  I approached her eagerly, somewhat like a youth approaching a kung-fu master for acceptance as a disciple.  Despite my sweaty clothes, she greeted me warmly and tried to satiate the curiosity in my plethora of questions.  She smiled at me, “With this type of thing, there’s a whole lot you can do.”  “I know,” I thought, “’a whole lot’ is exactly what I aspire to do.”  I thanked her and left the gym with a wondrously amplified respect for the technological world.&lt;/p&gt;
&lt;p&gt;I whispered to my friend, “Wouldn’t it be CRR-A-AZY if she used the computer to mix Bach and Gershwin?”  She nodded in simultaneous agreement and doubt, a doubt vanquished by Dabby’s climactic composition, the affectionately named “Gershbach.”   I had downloaded many techno remixes of classical songs, but the Gershbach was far beyond any conventional mixing.  I fell in love. &lt;/p&gt;
&lt;p&gt;The next day in my calculus class, the teacher, a fervent lover of classical music, explained the Lorenz system, or the tangled thread Professor Dabby had displayed.  The system was part of chaos theory, formally defined as the qualitative study of unstable aperiodic behavior in deterministic nonlinear dynamical systems; in other words, the study of patterns in seemingly patternless occurrences. The Lorenz system had been modeled on the movement of a bucket-waterwheel with holes in the buckets and water pouring from the top.  Every so often, the wheel would change direction due to the imbalance caused by the holes. After hearing this, I opened my mouth and kept it that way for an unusual amount of time.  I pictured the tangled thread and waterwheel. They looked quite weird and didn’t seem related at all.   Man had created music from a waterwheel.   If this was true, why couldn’t I create music from, say, the aimless, buzzing fly on my desk?  Since then, I respected math even more and became determined to master it, so that I might one day create something crazy and useful like the Lorenz system.  &lt;/p&gt;
&lt;p&gt;Really, this was all just the beginning.  Perhaps my excitement was naïve. Perhaps this math-music concept was old stuff.  But no matter how much had been done on the subject, there would be infinitely more to do…infinitely more that I could do.     I thought to myself, “Isn’t math a part of everything, including iguanas? Isn’t music a part of everything, including fire?  Isn’t everything a part of everything?”  I developed an ambition to create much with mathematics, and much with music.  For example, I could use math to create visual art or attach electrodes to my head to monitor my emotions and create music from the graphical representation of my sensations.  Heck, these kinds of activities became my goals in life.  And they were just the beginning.&lt;/p&gt;</content><category term="AI art"></category><category term="AI art"></category><category term="machine learning"></category><category term="music"></category></entry><entry><title>How Stable Diffusion (Latent Diffusion) works</title><link href="https://jerrychi.com/how-stable-diffusion-latent-diffusion-works.html" rel="alternate"></link><published>2022-08-25T00:00:00+09:00</published><updated>2022-08-25T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2022-08-25:/how-stable-diffusion-latent-diffusion-works.html</id><summary type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;UPDATE 2022–10–07: a new, more beginner-friendly …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;UPDATE 2022–10–07: a new, more beginner-friendly article on Stable Diffusion was posted at&lt;a href="https://jalammar.github.io/illustrated-stable-diffusion/"&gt;
https://jalammar.github.io/illustrated-stable-diffusion/&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;But how does it actually work from a technical perspective?&lt;/p&gt;
&lt;p&gt;Basically, it uses a variational autoencoder (VAE) combined with a denoising diffusion model. The key idea is that using diffusion models in pixel space (the raw image) is not the most efficient approach, since there are many barely perceptible small details that are not efficiently learned by a diffusion model. Rather, we can use a VAE to map images into a latent space (a form of compression), and then train the diffusion model in the (much smaller) latent space of images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image_alt_text" src="https://miro.medium.com/max/640/0*iltP4V3p0cGAkL_O"&gt;
Source: &lt;a href="https://huggingface.co/blog/stable_diffusion"&gt;https://huggingface.co/blog/stable_diffusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stable Diffusion is &lt;a href="https://github.com/CompVis/stable-diffusion"&gt;basically a special case / specific configuration of Latent Diffusion&lt;/a&gt;. A lot of effort went into making it very high-quality and easy to use for the masses.&lt;/p&gt;
&lt;p&gt;The above explanation barely scratches the surface. &lt;a href="https://docs.google.com/document/d/1x4iHe9mdyqpuINRN2EYMuG6_0JSBoNnjtDdSte18Ugc/edit#"&gt;&lt;strong&gt;For more in-depth details on Stable Diffusion / Latent Diffusion, please see this google doc I made.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also, I compiled various AI art resources (both technical and non-technical) at &lt;a href="https://tinyurl.com/creative-ai-links"&gt;https://tinyurl.com/creative-ai-links&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, we need to wrap up with some actual AI art :) The below image was generated using Stable Diffusion at &lt;a href="https://beta.dreamstudio.ai/"&gt;https://beta.dreamstudio.ai/&lt;/a&gt; with the prompt “Character portrait of a graceful and pretty Korean princess with gorgeous detailed eyes and flowing hair, fantasy setting, color page, tankobon, 4k, tone mapping, doll, akihiko yoshida, james jean, andrei riabovitchev, marc simonetti, yoshitaka amano”
&lt;img alt="image_alt_text" src="https://miro.medium.com/max/720/1*ePNHU-hG80IQrfE8m7J-1w.jpeg"&gt;&lt;/p&gt;</content><category term="AI art"></category><category term="AI art"></category><category term="machine learning"></category></entry><entry><title>Generating Fake K-pop Faces Part 1</title><link href="https://jerrychi.com/generating-fake-k-pop-faces-part-1.html" rel="alternate"></link><published>2019-02-11T00:00:00+09:00</published><updated>2019-02-11T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-02-11:/generating-fake-k-pop-faces-part-1.html</id><summary type="html">&lt;p&gt;·5 min read&lt;/p&gt;
&lt;p&gt;This project is for my own personal fun and learning. Hopefully it’ll be fun for some to read about my progress as well.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*b7Yy0V2cHo5n2yIZxlJMMw.gif"&gt;My first (poor) attempt to generate faces. Read on to see how I did this.&lt;/p&gt;
&lt;h1&gt;Planned approach&lt;/h1&gt;
&lt;p&gt;I expect this project to have …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·5 min read&lt;/p&gt;
&lt;p&gt;This project is for my own personal fun and learning. Hopefully it’ll be fun for some to read about my progress as well.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*b7Yy0V2cHo5n2yIZxlJMMw.gif"&gt;My first (poor) attempt to generate faces. Read on to see how I did this.&lt;/p&gt;
&lt;h1&gt;Planned approach&lt;/h1&gt;
&lt;p&gt;I expect this project to have maybe 7 steps. I’ve done steps 1 and 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;gather, filter, and normalize training data (images)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a simple autoencoder (neural network)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a variational autoencoder (VAE)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a vanilla generative adversarial network (GAN)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a &lt;a href="https://github.com/hindupuravinash/the-gan-zoo"&gt;fancier GAN architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;switch to using cloud GPUs instead of my Macbook (if I haven’t already) for better results; tune hyperparameters etc. a bit more&lt;/li&gt;
&lt;li&gt;try expanding to J-pop faces and/or male faces? (thanks to &lt;a href="https://medium.com/@daisukeishii"&gt;Daisuke Ishii&lt;/a&gt; of Team AI for the suggestion)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Step 1: gather, filter, and normalize images&lt;/h1&gt;
&lt;p&gt;This was actually a bit tricky, and I wanted to do it without any manual human filtering (because 1. it was a fun technical challenge for myself and 2. I wanted the process to be easily scalable to more images or other categories of people)&lt;/p&gt;
&lt;h2&gt;Downloading images&lt;/h2&gt;
&lt;p&gt;It was surprisingly easy to crawl/scrape images from Google, Bing, and Baidu image search given the Python package &lt;a href="https://pypi.org/project/icrawler/"&gt;icrawler&lt;/a&gt;. Here is a code snippet showing how easy this is.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;keywords = \[&amp;#39;걸그룹 프로필 사진&amp;#39;, &amp;#39;걸그룹 셀카&amp;#39;\] # e.g. &amp;quot;girl group profile pic&amp;quot;**for** keyword **in** keywords:  
    google\_crawler.crawl(keyword=keyword, max\_num=300, min\_size=(300,300), file\_idx\_offset=&amp;#39;auto&amp;#39;)  
    bing\_crawler.crawl(keyword=keyword, max\_num=100, min\_size=(300,300), file\_idx\_offset=&amp;#39;auto&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(full code &lt;a href="https://github.com/peacej/CADL/blob/master/kpop-faces/scrape_kpop_images.ipynb"&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I used multiple search engines and keyword phrases in different languages both to get more input data (images) and provide more variety in the input data (variety deriving both from algorithmic differences and cultural differences, i.e. Chinese journalists might depict K-pop a bit differently). Another reason for using Chinese keywords for Baidu image search: it seems that Baidu tends to do a poor job of providing relevant search results when searching in other languages.&lt;/p&gt;
&lt;h2&gt;Normalize/preprocess images&lt;/h2&gt;
&lt;p&gt;I won’t get into the details, but normalization involved mainly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;object localization and cropping (i.e. find the bounding box for a face in an image and crop to that box; sometimes this involved outputs of multiple face images per raw input image, since multiple people can be in one input image)&lt;/li&gt;
&lt;li&gt;resizing&lt;/li&gt;
&lt;li&gt;alignment (sometimes a face is tilted diagonally; I rotated the face so that eyes are perfectly horizontal)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I did all of the above by adapting existing code snippets and using the &lt;a href="https://opencv.org/"&gt;OpenCV&lt;/a&gt;, &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt;, and &lt;code&gt;[imutils](https://github.com/jrosebr1/imutils)&lt;/code&gt; packages.&lt;/p&gt;
&lt;h2&gt;Filter images&lt;/h2&gt;
&lt;p&gt;For the time being, I wanted to ensure that the input data includes only female faces, not male. Since gender classification is already a common ML task, I thought I would re-use an existing model. In fact, I ended up using TWO existing models (&lt;a href="https://modeldepot.io/harshsikka/gender-classification"&gt;1&lt;/a&gt;, &lt;a href="https://github.com/oarriaga/face_classification/blob/master/trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5"&gt;2&lt;/a&gt;) with different neural network architectures and only let images through if both models classified them as “female” above a certain probability threshold.&lt;/p&gt;
&lt;p&gt;One might point out that if I just re-use an existing gender classification model without understanding it, I might be unknowingly introducing bias. For example, if the model had been trained mainly on caucasian faces, then the model might only be able to output high probabilities of “female” when the face looked somewhat similar to a caucasian face (and the model might be less certain otherwise). Well, that’s OK for now, because 1. I’m just having fun here and 2. the result seemed to be a good representation of female K-pop faces (in terms of my own qualitative judgement).&lt;/p&gt;
&lt;p&gt;A further side benefit of using this gender classification filter is that, for images that are somehow weird or failed to be normalized properly (e.g. the the person is making a funny face or the picture is actually of an animal not a human), the gender classifier will tend to be uncertain about the gender classification, and hence such images would get filtered out by this filter.&lt;/p&gt;
&lt;p&gt;After normalization and filtering, the average image looked like the below. I was pleasantly surprised at how well the position of eyes matched across images. (In contrast, when I took the average of input images for another project years ago, the result was just a meaningless gray blob.) Seems like I did an OK job of normalizing and filtering :)&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*IC497K65IJlN4foz6DsMPg.png"&gt;The average input image&lt;/p&gt;
&lt;p&gt;Also, to more specifically visualize the variation across images (darker = less variation):&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*c7rKwRd9W2Zx7hFjHLkljg.png"&gt;Color = the mean across color channels of the standard deviation across images for each pixel&lt;/p&gt;
&lt;p&gt;Also, here’s a montage of a random subset of the input images (after normalization/filtering):&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*33bb0esPAC3JHVOU9PsZ1A.png"&gt;I’m sorry if this violates copyrights&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;Step 2: train and use a simple autoencoder&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;The autoencoder architecture I used was simple and conceptually similar to the below. Each column, a vector of values, is a layer in the neural network. We train the autoencoder by trying to make it produce an output image that is similar to each input image, which forces the “bottleneck” (middle layer) to capture the essence of the image in a very small vector of numbers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/0*H2au8b6rXfHwNSzp.png"&gt;Image stolen from &lt;a href="https://www.jeremyjordan.me/autoencoders/"&gt;jeremyjordan.me&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For now, I used only a vector of 2 values (elements) as the bottleneck in the middle. It turned out that, without any intentional design on my part, these 2 values ended up representing &lt;strong&gt;hair color&lt;/strong&gt; and &lt;strong&gt;horizontal face rotation&lt;/strong&gt;, respectively (or something close to that). I think a casual human observer would also qualitatively comment that these 2 factors (hair color and face rotation) are the primary cause of variance between images, so it’s cool that the neural network discovered the same factors automagically. This is one big use case for autoencoders (and some other ML algorithms) in general: you can boil down complex information to its essence, which can be represented simply.&lt;/p&gt;
&lt;p&gt;The animation at the top of this post combines the results from varying the 2 values in the bottleneck vector between -1 and 1 and then applying the decoder to those values.&lt;/p&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;The code is not super clean, but &lt;a href="https://github.com/peacej/CADL/tree/master/kpop-faces"&gt;here it is&lt;/a&gt;.&lt;br&gt;
A good part of the code was borrowed from &lt;a href="https://github.com/pkmital/CADL/tree/master/session-3"&gt;session 3&lt;/a&gt; of the class &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info"&gt;Creative Applications of Deep Learning Using Tensorflow on the e-learning platform Kadenze&lt;/a&gt;. It’s a great class, but it might be a bit out of date.&lt;/p&gt;
&lt;h1&gt;See you next time!&lt;/h1&gt;
&lt;p&gt;Please stay tuned for part 2 =D Hopefully I’ll be able to generate more realistic images next time.&lt;/p&gt;</content><category term="AI art"></category><category term="AI art"></category><category term="tutorial"></category><category term="computer vision"></category></entry></feed>