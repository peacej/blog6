<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jerry Chi's website</title><link href="https://jerrychi.com/" rel="alternate"></link><link href="https://jerrychi.com/feeds/all.atom.xml" rel="self"></link><id>https://jerrychi.com/</id><updated>2022-08-25T00:00:00+09:00</updated><subtitle>Jerry Chi&lt;BR&gt;Data Scientist in Tokyo</subtitle><entry><title>How Stable Diffusion (Latent Diffusion) works</title><link href="https://jerrychi.com/how-stable-diffusion-latent-diffusion-works.html" rel="alternate"></link><published>2022-08-25T00:00:00+09:00</published><updated>2022-08-25T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2022-08-25:/how-stable-diffusion-latent-diffusion-works.html</id><summary type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;But how does it actually work from a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·2 min read&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://huggingface.co/CompVis/stable-diffusion"&gt;Stable Diffusion model&lt;/a&gt;, just released a few days ago, is all the rage right now, with tons of people generating all sorts of amazing high-quality images, sometimes on par with or even better than OpenAI’s DALL-E 2.&lt;/p&gt;
&lt;p&gt;But how does it actually work from a technical perspective?&lt;/p&gt;
&lt;p&gt;Basically, it uses a variational autoencoder (VAE) combined with a denoising diffusion model. The key idea is that using diffusion models in pixel space (the raw image) is not the most efficient approach, since there are many barely perceptible small details that are not efficiently learned by a diffusion model. Rather, we can use a VAE to map images into a latent space (a form of compression), and then train the diffusion model in the (much smaller) latent space of images.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image_alt_text" src="https://miro.medium.com/max/20006/0*iltP4V3p0cGAkL_O"&gt;Source: &lt;a href="https://huggingface.co/blog/stable_diffusion"&gt;https://huggingface.co/blog/stable_diffusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stable Diffusion is &lt;a href="https://github.com/CompVis/stable-diffusion"&gt;basically a special case / specific configuration of Latent Diffusion&lt;/a&gt;. A lot of effort went into making it very high-quality and easy to use for the masses.&lt;/p&gt;
&lt;p&gt;The above explanation barely scratches the surface. &lt;a href="https://docs.google.com/document/d/1x4iHe9mdyqpuINRN2EYMuG6_0JSBoNnjtDdSte18Ugc/edit#"&gt;&lt;strong&gt;For more in-depth details on Stable Diffusion / Latent Diffusion, please see this google doc I made.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also, I compiled various AI art resources (both technical and non-technical) at &lt;a href="https://tinyurl.com/creative-ai-links"&gt;https://tinyurl.com/creative-ai-links&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, we need to wrap up with some actual AI art :) The below image was generated using Stable Diffusion at &lt;a href="https://beta.dreamstudio.ai/"&gt;https://beta.dreamstudio.ai/&lt;/a&gt; with the prompt “Character portrait of a graceful and pretty Korean princess with gorgeous detailed eyes and flowing hair, fantasy setting, color page, tankobon, 4k, tone mapping, doll, akihiko yoshida, james jean, andrei riabovitchev, marc simonetti, yoshitaka amano”&lt;/p&gt;
&lt;p&gt;&lt;img alt="image_alt_text" src="https://miro.medium.com/max/20000/1*ePNHU-hG80IQrfE8m7J-1w.jpeg"&gt;&lt;/p&gt;</content><category term="AI art"></category><category term="AI art"></category><category term="machine learning"></category></entry><entry><title>Easy Blog Migration From Medium To Your Own Site Using Python</title><link href="https://jerrychi.com/easy-blog-migration-from-medium-to-your-own-site-using-python.html" rel="alternate"></link><published>2022-08-17T00:00:00+09:00</published><updated>2022-08-17T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2022-08-17:/easy-blog-migration-from-medium-to-your-own-site-using-python.html</id><summary type="html">&lt;p&gt;·3 min read&lt;/p&gt;
&lt;p&gt;Recently I migrated my existing articles from Medium to my own new website (&lt;a href="https://jerrychi.com"&gt;https://jerrychi.com&lt;/a&gt;) in a &lt;strong&gt;quick, low-code way&lt;/strong&gt;. I wanted to have both blog articles and permanent pages (e.g. “About Me” page) and &lt;strong&gt;high customizability while spending minimal effort and money&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You can …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·3 min read&lt;/p&gt;
&lt;p&gt;Recently I migrated my existing articles from Medium to my own new website (&lt;a href="https://jerrychi.com"&gt;https://jerrychi.com&lt;/a&gt;) in a &lt;strong&gt;quick, low-code way&lt;/strong&gt;. I wanted to have both blog articles and permanent pages (e.g. “About Me” page) and &lt;strong&gt;high customizability while spending minimal effort and money&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You can do this too! Here’s how:&lt;/p&gt;
&lt;p&gt;Register a domain name. I used &lt;a href="https://domains.google/"&gt;https://domains.google/&lt;/a&gt; to register &lt;a href="https://jerrychi.com"&gt;jerrychi.com&lt;/a&gt;, paying about $12 per year. This is the only thing I’m paying for.&lt;/p&gt;
&lt;p&gt;Install required Python libraries including &lt;a href="https://getpelican.com/"&gt;Pelican&lt;/a&gt; (the Python-based static site generator for creating personal blogs/websites) on your local laptop. E.g. &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; (see my &lt;code&gt;[requirements.txt](https://github.com/peacej/blog6/blob/main/requirements.txt)&lt;/code&gt;). I used Python 3.8.6 in a &lt;a href="https://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html"&gt;conda environment&lt;/a&gt; but other setups should work too.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image_alt_text" src="https://miro.medium.com/max/2000/1*F-NLYAtzFjXLDYa4PLzXOw.png"&gt;Pelican logo&lt;/p&gt;
&lt;p&gt;Auto-convert any Medium articles you choose to Markdown files (which are compatible with Pelican, which will then auto-convert them to HTML/Javascript) using the &lt;code&gt;medium_to_markdown&lt;/code&gt; tool described at &lt;a href="https://willkoehrsen.github.io/writing/markdown/converting-medium-posts-to-markdown-for-your-blog/"&gt;https://willkoehrsen.github.io/writing/markdown/converting-medium-posts-to-markdown-for-your-blog /&lt;/a&gt; . I did &lt;a href="https://github.com/peacej/blog6/blob/main/medium_to_markdown.py"&gt;slightly tweak his code&lt;/a&gt; to e.g. filter out a few specific words. This doesn’t require any special access to your Medium account; it’s just scraping the text from the public internet.&lt;/p&gt;
&lt;p&gt;Push your code to a new Github repo.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can host the website on &lt;a href="https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages"&gt;Github Pages&lt;/a&gt; for free, up to 1 GB of storage (nothing beyond that even if you want to pay). If I ever need more than 1 GB, I might migrate to Heroku.&lt;/li&gt;
&lt;li&gt;You can &lt;a href="https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site"&gt;use your own custom domain&lt;/a&gt; (in my case &lt;a href="https://jerrychi.com"&gt;jerrychi.com&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need a &lt;code&gt;master&lt;/code&gt; or &lt;code&gt;main&lt;/code&gt; git branch for all the code and then a &lt;code&gt;gh-pages&lt;/code&gt; branch to store the actual website assets that are served when people visit your site. Thankfully one can just use a tool called &lt;code&gt;ghp-import&lt;/code&gt; to automate pushing the only the needed to &lt;code&gt;gh-pages&lt;/code&gt; branch. You can see &lt;a href="https://github.com/peacej/blog6/blob/main/buildpush.sh"&gt;how I use this tool&lt;/a&gt;. After pushing to github, usually changes are reflected in a few seconds on my website.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tip: style changes to CSS files etc. may require an “&lt;a href="https://gsuitetips.com/tips/chrome/chrome:-empty-cache-and-hard-reload/#:~:text=Normal%20Reload%3A%20Uses%20Cached%20Data,be%20re%2Ddownloaded%20as%20required."&gt;empty cache and hard reload&lt;/a&gt;” in Chrome to be visually reflected on the website due to caching.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pelican also supports add-ons, themes, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See the &lt;a href="https://github.com/getpelican/pelican-themes"&gt;pelican-themes repo&lt;/a&gt; for instructions on themes.&lt;/li&gt;
&lt;li&gt;I chose to use the &lt;a href="https://github.com/alexandrevicenzi/Flex/"&gt;Flex theme&lt;/a&gt; which includes many nifty features such a mobile-first responsive UI. I cloned this Flex repo directly rather than cloning from the &lt;code&gt;pelican-themes&lt;/code&gt; repo which pins Flex at an older version.&lt;ul&gt;
&lt;li&gt;In &lt;code&gt;pelicanconf.py&lt;/code&gt; (the main configuration file) I simply set &lt;code&gt;PLUGINS = ["pelican.plugins.search"]&lt;/code&gt; and &lt;code&gt;STORK_VERSION = "1.5.0"&lt;/code&gt; to enable a search box on the left navigation bar. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I also edited the &lt;code&gt;pelican-templates/Flex/static/stylesheet/style.less&lt;/code&gt; file to customize the font style (my first time dealing with the &lt;a href="https://lesscss.org/"&gt;Less language&lt;/a&gt; but it was easy to edit).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s it! Now you have a nice shiny new personal website~&lt;/p&gt;
&lt;p&gt;So, why did I choose Pelican? I also considered Nikola and Django:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nikola: the second-most popular Python static site generator after Pelican. I did initially try building with Nikola but got frustrated with the documentation being a bit unclearly worded and the lack of resources/answers when e.g. googling around for an issue. Pelican is more established and popular with more discussion/answers for potential issues.&lt;/li&gt;
&lt;li&gt;Django: the most popular Python web backend framework. I did take the &lt;a href="https://docs.djangoproject.com/en/4.1/intro/tutorial01/"&gt;Django official tutorial&lt;/a&gt;, which was enjoyable and extremely well-written. However, Django is much more complicated than Pelican (which is natural since Django is a general-purpose framework for creating any sort of webpage) and I’d likely end up combining Django with a frontend framework like React (meaning one more framework to learn). The complexity and work required was overkill for needs: a simple personal website. I’ll come back to learn Django+React if I need a much fancier site.&lt;/li&gt;
&lt;/ul&gt;</content><category term="web dev"></category><category term="web dev"></category><category term="blogging"></category><category term="Python"></category></entry><entry><title>How Machine Learning May Save Humanity</title><link href="https://jerrychi.com/how-machine-learning-may-save-humanity.html" rel="alternate"></link><published>2020-06-12T00:00:00+09:00</published><updated>2020-06-12T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2020-06-12:/how-machine-learning-may-save-humanity.html</id><summary type="html">&lt;p&gt;·4 min read&lt;/p&gt;
&lt;p&gt;I think humanity will end eventually (whether it takes hundreds or thousands of years). So what’ll we do when the end is near? We’d want to leave our legacy; in other words, &lt;strong&gt;we’d want to save humanity’s knowledge and history as data and …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;·4 min read&lt;/p&gt;
&lt;p&gt;I think humanity will end eventually (whether it takes hundreds or thousands of years). So what’ll we do when the end is near? We’d want to leave our legacy; in other words, &lt;strong&gt;we’d want to save humanity’s knowledge and history as data and send it into space, hoping that aliens somewhere will receive it&lt;/strong&gt;. That sounds a lot better than just disappearing into the void without a trace.&lt;/p&gt;
&lt;p&gt;This begs the questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;what information to send&lt;/li&gt;
&lt;li&gt;how to compress/represent the information/data&lt;/li&gt;
&lt;li&gt;how to send it (including the &lt;a href="https://arxiv.org/pdf/1101.4968.pdf"&gt;transmission strategy&lt;/a&gt;: using lasers vs. omnidirectional radio transmitters, direction to point in, etc.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In fact, the approach for #2 (compression) affects #1 and #3. For example, we could decide to select more information for sending if we know it can be efficiently compressed, and if the compression algorithm allows fault-tolerant decompression, that might allow aliens to extract useful information even with incomplete reception of the transmitted data.&lt;/p&gt;
&lt;h1&gt;The compression tradeoff&lt;/h1&gt;
&lt;p&gt;We’d want to convey useful information on the &lt;strong&gt;essence&lt;/strong&gt; of humanity with &lt;strong&gt;lossy compression&lt;/strong&gt; (i.e. we aren’t able to perfectly recover the original data when decompressing). I speculate that the tradeoff between useful information and lossiness of compression may look something like the below, so we’d want to target the highest point in the curve.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*peVnP6QaPft--FpKzvLveA.png"&gt;(&lt;a href="https://docs.google.com/spreadsheets/d/1bvpLlxeYF_njHC4oPDhXIgC7bGO5ZLejPPKl9OxFGTw/edit#gid=0"&gt;spreadsheet for the above&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;As you increase lossiness, the information, when decompressed/recovered, becomes less true to the original data and thus less “useful,” but it allows for more efficient compression algorithms, which allows you to fit more usefulness into each bit. The definition of what is “useful” info about humanity is a philosophical debate which I won’t get into now.&lt;/p&gt;
&lt;h1&gt;Machine learning to the rescue&lt;/h1&gt;
&lt;p&gt;Embeddings (essentially, a list of numbers) generated by deep neural networks (e.g. &lt;a href="https://medium.com/@peacej2/generating-fake-k-pop-faces-part-1-6202dc27f0eb"&gt;autoencoders&lt;/a&gt; or language models such as &lt;a href="http://jalammar.github.io/illustrated-bert/"&gt;BERT&lt;/a&gt;) can be seen as a form of lossy compression (where the degree of compression can be arbitrarily chosen). For example, you can represent an image or the profile of a human with an embedding. As &lt;a href="https://arxiv.org/abs/1206.5538"&gt;representation learning&lt;/a&gt; techniques advance, perhaps we’ll be able to accurately represent a human’s behavior/personality with an embedding (queue &lt;a href="https://www.youtube.com/watch?v=rYelEUVQ50g"&gt;Westworld theme song&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Via training, a neural network can figure out how to efficiently represent the essence of the information. Such an approach is &lt;a href="https://www.forbes.com/sites/nvidia/2019/02/11/accelerating-the-internet-with-ai-based-media-compression/#1a6508107760"&gt;already being used to reduce video download sizes&lt;/a&gt;. In the future, we’ll have even better machine learning techniques to do this even more efficiently.&lt;/p&gt;
&lt;p&gt;Of course, we’ll need to send not just the embeddings, but also the neural network parameters as well so that the aliens can make sense of the embeddings.&lt;/p&gt;
&lt;p&gt;The information that is represented isn’t limited to just words, images, and numbers. A generative neural network that can accurately generate random new virtual humans (i.e. it “samples” from a realistic distribution of human physique and behavior) or simulate human movement in cities may also be “information” that we want to send.&lt;/p&gt;
&lt;h1&gt;It’s t̶u̶r̶t̶l̶e̶s̶ models all the way down&lt;/h1&gt;
&lt;p&gt;Let’s get meta for a moment. Neural networks themselves are information. In fact, &lt;a href="https://arxiv.org/abs/1906.04358"&gt;even neural networks with randomly chosen parameters can perform well&lt;/a&gt;, which implies that the architecture of the network is also important information. Furthermore, a single neural network can be a lot of information. For example, &lt;a href="https://medium.com/@Synced/openai-unveils-175-billion-parameter-gpt-3-language-model-3d3f453124cd"&gt;OpenAI’s GPT-3 language model has 175 billion parameters&lt;/a&gt;. Hundreds of years from now, we might have models with quadrillions of parameters (which could… simulate all of human civilization…?)&lt;/p&gt;
&lt;p&gt;So, we might want to send the aliens:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the parameters of a neural network compressed using another neural network&lt;/li&gt;
&lt;li&gt;a model which can design/train other models (although getting this to run on an alien’s hardware/software stack might be non-trivial)&lt;/li&gt;
&lt;li&gt;one of the above recursed indefinitely&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(BTW, &lt;a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down"&gt;see this&lt;/a&gt; if you didn’t get the joke about turtles 😊)&lt;/p&gt;
&lt;h1&gt;What would aliens do with the data?&lt;/h1&gt;
&lt;p&gt;Hard to say. Maybe they would benefit from some of our technology. Maybe they’d create a computer simulation of earth and humans living on it. Or maybe they’d using advanced bio-engineering to create living things resembling humans… In any case, it would be better than no one ever knowing that humans existed.&lt;/p&gt;
&lt;h1&gt;This might be the most important thing we ever do&lt;/h1&gt;
&lt;p&gt;Do you want to prioritize big, long-term impact over short-term quick wins? If so, let’s extend the thinking by a few years (or millenia). &lt;strong&gt;After humanity perishes, our long-term impact will be essentially zero unless we communicate with extraterrestrial intelligence.&lt;/strong&gt; Unless, of course, we create robots that outlive us in a meaningful way…but even then, the physical robots may perish eventually due the sheer distance to inhabitable planets… then, the only way to &lt;strong&gt;save&lt;/strong&gt; our legacy is via efficient transmission of information through space. Otherwise, &lt;strong&gt;the universe will be as if humans had never existed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*7y_jTOIX7frtWVz5iSoBPQ.png"&gt;Image from &lt;a href="https://art.marcsimonetti.com/science-fiction"&gt;https://art.marcsimonetti.com/science-fiction&lt;/a&gt;&lt;/p&gt;</content><category term="machine learning"></category><category term="machine learning"></category><category term="futurism"></category><category term="computer science"></category></entry><entry><title>The Nature Of Human Intelligence From A Computer Science Perspective</title><link href="https://jerrychi.com/the-nature-of-human-intelligence-from-a-computer-science-perspective.html" rel="alternate"></link><published>2019-10-13T00:00:00+09:00</published><updated>2019-10-13T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-10-13:/the-nature-of-human-intelligence-from-a-computer-science-perspective.html</id><summary type="html">&lt;p&gt;9 min read&lt;/p&gt;
&lt;p&gt;I’m tremendously curious about the long-term future of artificial intelligence. Understanding human intelligence better may be key to predicting the future of artificial intelligence. Thanks to both old books and recent research, my thinking and predictions have evolved quite a bit in the last few years …&lt;/p&gt;</summary><content type="html">&lt;p&gt;9 min read&lt;/p&gt;
&lt;p&gt;I’m tremendously curious about the long-term future of artificial intelligence. Understanding human intelligence better may be key to predicting the future of artificial intelligence. Thanks to both old books and recent research, my thinking and predictions have evolved quite a bit in the last few years. I now believe that Strong AI will be realized.&lt;/p&gt;
&lt;p&gt;Regarding human intelligence, I now believe that the following statements are probably true. Some of this is just speculation. (For brevity, I’ll omit the words “probably” and “in my opinion” for the rest of the article.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All or almost all of human intelligence can be deemed to be some form of computation or data input/output/storage.&lt;/li&gt;
&lt;li&gt;Human intelligence doesn’t just come from the brain. Actually, almost every part of the human body (and even bacteria etc.) plays a role in human intelligence.&lt;/li&gt;
&lt;li&gt;The whole is much greater than the sum of its parts. E.g. each cell has limited intelligence by itself, but together, great intelligence can be achieved.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Parallel&lt;/strong&gt; computation at multiple levels is key and the parallelism is in a way even more impressive than a data center full of GPUs.&lt;/li&gt;
&lt;li&gt;Computation is &lt;strong&gt;distributed&lt;/strong&gt; and &lt;strong&gt;decentralized&lt;/strong&gt; (like blockchains 😜); there is no centralized component. There is no human analogue of a CPU controlling a whole computer or master node controlling a cluster of computers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursion / feedback&lt;/strong&gt; &lt;strong&gt;loops / self-improvement&lt;/strong&gt; are also important at multiple levels&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attention&lt;/strong&gt; and &lt;strong&gt;abstraction&lt;/strong&gt; mechanisms greatly decrease the amount of computation required (i.e. they improve the bang for the buck for each unit of computation)&lt;/li&gt;
&lt;li&gt;There are many ways that parts of the human brain/body execute computer science algorithms or mathematical operations&lt;/li&gt;
&lt;li&gt;Human intelligence makes use of both machine learning algorithms and logical reasoning (&lt;a href="https://en.wikipedia.org/wiki/Computer_algebra"&gt;symbolic computation&lt;/a&gt; etc.)&lt;/li&gt;
&lt;li&gt;Human intelligence would be very difficult to achieve using current computer hardware architecture; there must be a better approach. The human body/brain takes advantage of biology, chemistry, and physics in a way that normal computers can’t (yet).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s dig into each of these points in more detail.&lt;/p&gt;
&lt;h2&gt;Intelligence is just computation and data input/output/storage&lt;/h2&gt;
&lt;p&gt;In other words, I believe in &lt;a href="https://en.wikipedia.org/wiki/Functionalism_(philosophy_of_mind)"&gt;&lt;strong&gt;functionalism&lt;/strong&gt;&lt;/a&gt;. I think rejecting functionalism would mean that you’d need to believe in some concept of a soul or some other non-tangible/non-physical phenomena (which you’d never be able to verify). This relates to the nature of human consciousness. For a nice overview, see &lt;a href="http://omgwai.com/itsdho/Dennett_2001.pdf"&gt;&lt;em&gt;Are We Explaining Consciousness yet? (2001 paper by Daniel Dennett)&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Intelligence doesn’t just come from the brain&lt;/h2&gt;
&lt;p&gt;I used to think that all intelligence comes from the brain, but now I think that is wrong. Let’s consider a thought experiment: if we were able to surgically remove my brain and place it into a robot’s body, would I think/behave/feel the same way? No, I don’t think so (unless we have advanced technology that can synthesize robots that function exactly like human bodies).&lt;/p&gt;
&lt;p&gt;In other words, our bodies are part of our intelligence. For example, recent research shows that &lt;a href="http://The whole is much greater than the sum of its parts."&gt;the gut and its microbiome has significant affects on the brain and human behavior.&lt;/a&gt; A finger touching a hot surface will instantly retract before any electric signal reaches the brain. White blood cells wage war against invaders without instructions from the brain. Cells move around, release chemicals, and replicate following something like a computer program (DNA). It doesn’t matter if cells are unconscious; they contribute to the overall intelligence of the human. Also, did you know that &lt;a href="https://www.youtube.com/watch?v=ooA0J6DWWTM&amp;amp;t=13m19s"&gt;a single cell can solve mazes and difficult computer science problems&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;Humans don’t just gain intelligence by reading books or listening to lectures. Feeling the world through all senses and sensations (consciously and unconsciously) helps a human understand the world and act intelligently in that world.&lt;/p&gt;
&lt;h2&gt;The whole is much greater than the sum of its parts&lt;/h2&gt;
&lt;p&gt;One human is simply the “animal” level of the &lt;a href="https://waitbutwhy.com/2019/10/idea-labs-echo-chambers.html"&gt;&lt;strong&gt;Emergence Tower&lt;/strong&gt;&lt;/a&gt;, where each level is constituted by combining several instances of the level below it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*kJXRYzvsw55xVqAgz7T87w.png"&gt;from &lt;a href="https://waitbutwhy.com/2019/10/idea-labs-echo-chambers.html"&gt;https://waitbutwhy.com/2019/10/idea-labs-echo-chambers.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An interesting analogy is how an ant colony is collectively much more intelligent than the sum of each constituent ant’s intelligence; this is perhaps &lt;a href="https://qr.ae/TWYnOU"&gt;described most entertainingly in the famous book &lt;em&gt;G&lt;/em&gt;&lt;/a&gt;ö&lt;a href="https://qr.ae/TWYnOU"&gt;&lt;em&gt;del, Escher, and Bach&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Related concepts are the &lt;a href="https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds"&gt;&lt;strong&gt;wisdom of crowds&lt;/strong&gt;&lt;/a&gt; and computer science concepts such as &lt;strong&gt;evolutionary algorithms&lt;/strong&gt;,  &lt;a href="https://www.lamsade.dauphine.fr/~lang/papers/sofsem07.pdf"&gt;&lt;strong&gt;computational social choice&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;graph computing&lt;/strong&gt;, and &lt;strong&gt;neural networks&lt;/strong&gt; (where &lt;a href="https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f"&gt;multiple neural networks combined&lt;/a&gt; can be more “intelligent” or useful than the simple sum of each constituent network’s intelligence, and one network can be deemed more intelligent than the sum of each constituent layer’s intelligence).&lt;/p&gt;
&lt;h2&gt;Parallel computing&lt;/h2&gt;
&lt;p&gt;If we assume most of the &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3842595/"&gt;cells in the human body can each be deemed to have some sort of computational capacity&lt;/a&gt; (a single cell might be dumb and slow by itself, but that’s besides the point), then that implies one human can make 10s of trillions of computations simultaneously (in &lt;strong&gt;parallel&lt;/strong&gt;). Compare that with traditional computers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A traditional CPU can only process one thing at a time.&lt;/li&gt;
&lt;li&gt;A multithreading CPU might be able to handle 64 simultaneous computations.&lt;/li&gt;
&lt;li&gt;A decent GPU might be able to handle &lt;a href="https://stackoverflow.com/questions/6490572/cuda-how-many-concurrent-threads-in-total"&gt;tens of thousands of simultaneous computations&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A data warehouse full of GPUs might be able to handle 100 million simultaneous computations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parallel computing is key because it allows humans to e.g. react to things very quickly. E.g. if you saw an object flying towards your head, you would immediately try to dodge, not process each retina cell’s data one by one before acting. Of course, humans can’t apply efficient parallel processing to any arbitrary computation; humans are set up (via evolution) to use parallel processing well only for certain types of tasks.&lt;/p&gt;
&lt;h2&gt;Distributed and decentralized computation&lt;/h2&gt;
&lt;p&gt;This is true both for the body overall and for the brain. If we think of each computational unit (e.g. a cell, an organ, a piece of the brain) as an individual agent, then the agents are both &lt;strong&gt;competing&lt;/strong&gt; and &lt;strong&gt;collaborating&lt;/strong&gt;. At any given time, your gut might be competing with part of your brain to decide on the next feeling/thought/action, and one part of your brain could collaborate with another part to form a fuller thought, and then compete with yet another part of the brain to enter your consciousness and dominate your thought. Such collaborative and competitive mechanisms are what enable the whole to be greater than the sum of its parts (as described above).&lt;/p&gt;
&lt;p&gt;Some would argue that the thalamus / prefrontal cortex acts as a centralized processor, but this is &lt;a href="https://www.researchgate.net/post/Is_the_thalamus_REALY_the_CPU_of_the_Brain"&gt;debatable&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Recursion / feedback loops / self-improvement&lt;/h2&gt;
&lt;p&gt;There are at least 2 ways to apply this concept.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The brain seems &lt;a href="http://www.letras.ufrj.br/poslinguistica/recursion/papers/17-friederici.pdf"&gt;set up to perform tasks requiring recursive logic&lt;/a&gt; (e.g. speaking or understanding phrases like “Peter knew that Maria who loved Hans contacted Johann”)&lt;/li&gt;
&lt;li&gt;The brain is able to &lt;a href="https://www.bbc.com/future/article/20151123-the-brains-miracle-superpowers-of-self-improvement"&gt;self-improve&lt;/a&gt;. I.e. part of neuroplasticity is realized via the brain changing itself (indirectly?). For example, having some thoughts can &lt;a href="https://www.scirp.org/journal/PaperInformation.aspx?PaperID=73936"&gt;result in some enzymes being released, which then cause changes in the number of synapses and synaptic connections&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I have a feeling that feedback loops are at play in other ways as well.&lt;/p&gt;
&lt;p&gt;By the way, many people believe that self-improving AI is the key to better AI (and to conscious AI and Strong AI). From a practical standpoint, AI that writes or helps write AI software is a great example (as &lt;a href="http://www.youtube.com/watch?v=BH4ItyTVMEs&amp;amp;t=42m46s"&gt;François Chollet, the creator of Keras, believes&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Attention and Abstraction&lt;/h2&gt;
&lt;p&gt;Humans are good at minimizing wasteful computations. This follows naturally from evolution. A human might have 10 objects in her visual field, and 10 details about each of those objects. But human would naturally 1. limit &lt;strong&gt;attention&lt;/strong&gt; to only one of those objects (e.g. attractive potential mate or a dangerous rock flying towards one’s face) and 2. make a simple &lt;strong&gt;abstraction&lt;/strong&gt; of the object while ignoring the details (e.g. understand that it’s a dangerous, fast-moving object — no need to think about the exact shape or patterns on the rock). Attention and abstraction mechanisms can sometimes be weaknesses (e.g. sometimes humans jump to conclusions prematurely or gloss over important details) but most of the time they contribute positively to human intelligence.&lt;/p&gt;
&lt;p&gt;Computers can use similar approaches. The famous 2017 paper &lt;a href="https://arxiv.org/abs/1706.03762"&gt;&lt;em&gt;Attention is All You Need&lt;/em&gt;&lt;/a&gt;  led to huge advances in natural language processing. The 2018 paper &lt;a href="https://arxiv.org/abs/1803.10122"&gt;&lt;em&gt;World Models&lt;/em&gt;&lt;/a&gt;  described advances in using neural networks to create &lt;strong&gt;abstract&lt;/strong&gt; models of the world that are computationally tractable and combining them with other neural networks to perform tasks well.&lt;/p&gt;
&lt;h2&gt;Computer science algorithms and mathematical operations&lt;/h2&gt;
&lt;p&gt;For example, the &lt;a href="https://medium.com/human-like-machine-hearing-with-ai-1-3-a5713af6e2f8"&gt;ear amplifies certain sound frequencies and then acts as multiple bandpass filters&lt;/a&gt; to separate sounds into components. The visual cortex performs multi-layer convolutions (kind of like pooling pixels together) which inspired convolutional neural networks (CNNs), a core part of modern AI for image processing. (See the &lt;a href="http://www.deeplearningbook.org/contents/convnets.html"&gt;Neuroscientific Basis for Convolutional Networks on page 358 of the Deep Learning book&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;You could view such phenomena either as “the body uses such mathematical approaches” or, reversing the order, you could say “these mathematical models describe the body’s functionality” or “these algorithms are inspired by biology” but I don’t think it matters which view you choose; my main point is that the body can do cool stuff which contributes to human intelligence.&lt;/p&gt;
&lt;p&gt;For more on this, check out the 2012 paper &lt;a href="https://www.currentscience.ac.in/Volumes/103/04/0370.pdf"&gt;&lt;em&gt;Computation Algorithms Inspired by Biological Processes&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Machine learning algorithms and symbolic reasoning&lt;/h2&gt;
&lt;p&gt;Broadly, there are 2 major approaches to AI:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;statistical machine learning (learning from the data, e.g. learning to differentiate cats from dogs by looking at the pixels of many labeled photos)&lt;/li&gt;
&lt;li&gt;symbolic AI / logical reasoning / &lt;a href="https://en.wikipedia.org/wiki/Computer_algebra"&gt;computer algebra&lt;/a&gt; (e.g. manipulate 2y=x into y=0.5x or deduce from “all men are mortal” and “John is a man” that “John is mortal.”)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think that humans use some form of both of the above. Only using one of the above would be insufficient to explain the intelligence humans have demonstrated. Statistical machine learning is nowadays often implemented via neural networks which are inspired by the human brain. As for symbolic AI, I guess humans must also be able to use something similar to that, since humans are apparently able to do algebraic manipulations and make logical deductions pretty quickly.&lt;/p&gt;
&lt;p&gt;The question is, how exactly do we combine #1 and #2 ? There are many possible approaches. For example, see this video: &lt;a href="https://www.youtube.com/watch?v=ETHrFxiFIUM"&gt;Bridging Machine Learning and Logical Reasoning by Abductive Learning&lt;/a&gt; (2019). In this approach, “ machine learning models learn to perceive primitive logical facts from the raw data, while logical reasoning is able to correct the wrongly perceived facts for improving the machine learning model.” Does the human brain do something similar? Maybe. Another approach is &lt;a href="https://medium.com/@ybergquist/opencog-vs-openai-2-different-paths-to-human-level-ai-d36deafbf966"&gt;OpenCog&lt;/a&gt;, which includes both symbolic AI and neural networks as part of a larger connected “hypergraph.”&lt;/p&gt;
&lt;h2&gt;Taking advantage of biology, chemistry, and physics in the real world&lt;/h2&gt;
&lt;p&gt;A laughable but illustrative example is the &lt;a href="https://en.wikipedia.org/wiki/Spaghetti_sort"&gt;Spaghetti Sort algorithm&lt;/a&gt; which can be implemented by a human but not by a computer without a physical body. The algorithm involves holding a bunch of uncooked spaghetti rods of different lengths and slamming down the bunch against a table. A human can sort the rods by length with O(n) time complexity. In contrast, the fastest computer algorithm for sorting a list of numbers is O(n log n), which is worse. The computer is not able to take advantage of e.g. gravity which constantly acts on all the rods at the same time (&lt;strong&gt;in parallel&lt;/strong&gt;). Of course you could use a computer to simulate gravity’s effect on all the objects, but that is computationally expensive.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/0*VPTNlchpXsp39-XW.jpg"&gt;A human performing Spaghetti Sort&lt;/p&gt;
&lt;p&gt;An analogue to Spaghetti Sort is &lt;a href="https://interestingengineering.com/what-is-dna-computing-how-does-it-work-and-why-its-such-a-big-deal"&gt;DNA computing&lt;/a&gt;. “The power of DNA computing comes from its ability to perform the same operation &lt;strong&gt;simultaneously&lt;/strong&gt; on the contents of a test tube…operations can be performed &lt;strong&gt;in parallel&lt;/strong&gt; with no added cost” (from &lt;a href="http://www.cim.mcgill.ca/~scott/RIT/dna_computing.html"&gt;this paper&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In other words, stuff (deemed to be computations) happening in the real world (in human bodies) is naturally massively parallel (all happening at the same time) and this is a &lt;strong&gt;huge advantage of humans over traditional computers&lt;/strong&gt; (which naturally process things one by one and incur cost to enable parallelism — parallelism which pales in comparison to the real world).&lt;/p&gt;
&lt;p&gt;The above may be the biggest challenge to achieving Strong AI via electronic hardware (if we refrain from speculating about &lt;a href="https://www.quora.com/Can-quantum-computers-help-with-developing-strong-AI"&gt;using quantum computing for AI&lt;/a&gt;).&lt;/p&gt;
&lt;h1&gt;Closing&lt;/h1&gt;
&lt;p&gt;The more I learn about human intelligence, the more I appreciate all the things it can do. But with the flood of awesome research related to neuroscience and ML/AI (and we’re just getting started), and since we’re getting an increasingly better understanding of the mechanisms of human intelligence, &lt;strong&gt;I do believe that machine intelligence will be able to replicate or supersede many aspects of human intelligence eventually.&lt;/strong&gt; Not sure when exactly.&lt;/p&gt;
&lt;h1&gt;Disclaimers&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;I’m not an expert on any of these topics. I’m just a random curious dude.&lt;/li&gt;
&lt;li&gt;I purposefully avoided discussing the definition of intelligence; that could be a long philosophical debate. You can choose what it means to you, and hopefully the above will still make sense.&lt;/li&gt;
&lt;/ul&gt;</content><category term="computer science"></category><category term="computer science"></category><category term="human intelligence"></category></entry><entry><title>Guide To Working With Me A Data Science Manager</title><link href="https://jerrychi.com/guide-to-working-with-me-a-data-science-manager.html" rel="alternate"></link><published>2019-09-26T00:00:00+09:00</published><updated>2019-09-26T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-09-26:/guide-to-working-with-me-a-data-science-manager.html</id><summary type="html">&lt;p&gt;·9 min read&lt;/p&gt;
&lt;p&gt;This document is mainly for sharing with my colleagues and anyone I might work with in the future. But hey, I might as well post it on Medium; maybe someone will find it useful. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Below are principles, values, and best practices I tell myself to follow&lt;/strong&gt; (both …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·9 min read&lt;/p&gt;
&lt;p&gt;This document is mainly for sharing with my colleagues and anyone I might work with in the future. But hey, I might as well post it on Medium; maybe someone will find it useful. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Below are principles, values, and best practices I tell myself to follow&lt;/strong&gt; (both for working at organizations and working in data science). You don’t need to believe all the same principles, but &lt;strong&gt;if you read them you’ll understand how I think, and that may help us cooperate effectively.&lt;/strong&gt; I’ll try to understand how you think too, and we can work out our differences :)&lt;/p&gt;
&lt;h1&gt;Principles / best practices for work&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Almost nothing is absolute&lt;/strong&gt; (including all the below principles). When applying any belief or principle, one should consider exceptions and/or consider moderation/balance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Always be open to change.&lt;/strong&gt; The world and situations change quickly, and so people should change too. That could include changing my below principles.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Focus and prioritize&lt;/strong&gt;. If one is working on 5 major projects at the same time, that’s probably too many to be very effective. Clear &lt;strong&gt;deprioritization&lt;/strong&gt; is a necessary part of good prioritization. In some cases, consider implementing ruthless prioritization (say no — politely, of course — to important projects/people because you have even more important work).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be a driving force.&lt;/strong&gt; Drive projects and tasks forward and achieve progress in the face of uncertainty/ambiguity. Related concepts are leadership, proactiveness, and taking ownership. But be careful of fatigue or spreading yourself too thin while driving many projects (hence the need for focus).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be free.&lt;/strong&gt; We are all adults. We should be able to live and work the way we want to. This includes our work hours, the technical tools or Python/R libraries we want to use, etc. However, someone else should step in when someone is abusing their freedom and not producing good work as a result. Also, there are some cases where limiting freedom (by establishing a standardized, scalable approach) can lead to greater organizational effectiveness, so there’s a tradeoff to consider. But in general, I like to err towards the side of more freedom and creating soft guidelines instead of hard rules.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be open and transparent&lt;/strong&gt; (except if it could cause major issues… e.g. legal issues)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Be open-minded.&lt;/strong&gt; Most  people tend to become less accepting of conflicting approaches/ideas as they gain experience/expertise (and many are not self-aware that they’ve become less open-minded). Don’t let this happen to you.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keep people motivated,&lt;/strong&gt; including yourself and the people working with you. If people are not motivated, they probably won’t do good work or they’ll eventually leave the team or company. Take time to think about what motivates you and others (including career goals), and push for change if there’s a motivation problem. There are many reasons for low motivation; you should try to do something about it ASAP if possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Don’t micromanage. Err on the side of over-empowering others.&lt;/strong&gt; It’s tricky to decide how much responsibility/authority/independence to &lt;strong&gt;delegate&lt;/strong&gt; to someone. &lt;strong&gt;The best leaders delegate well&lt;/strong&gt;. I think in most cases it’s better to give too much than too little; empowering people leads to higher motivation, faster work progress, faster personal growth, better long-term results, better team dynamics, and warm fuzzy feelings. Most mistakes can be fixed/dealt with. Of course there are exceptions (don’t let the new intern perform heart surgery all by herself) and it depends somewhat on each individual’s preferences. My thinking is influenced by the &lt;a href="https://www.nbforum.com/nbreport/ilkka-paananen-the-least-powerful-ceo/"&gt;culture of my previous employer, Supercell&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://schoolofherring.com/2015/07/19/ilkka-paananen-of-supercell-celebrate-learnings-from-failures/"&gt;&lt;strong&gt;Celebrate failure&lt;/strong&gt;&lt;/a&gt;. It helps encourage continued risk-taking. But don’t &lt;a href="https://www.entrepreneur.com/article/247933"&gt;fetishize failure&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Don’t only focus on the short term&lt;/strong&gt;. Sure, there might be quarterly goals, and those are important, but most great accomplishments in human history took more than a quarter to achieve. Consider blocking time each week for a longer-term goal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Principles for good communication&lt;/h1&gt;
&lt;p&gt;Most communication / interpersonal problems could be avoided if everyone did just the first two below things well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stand in the shoes of your audience.&lt;/strong&gt; Before hitting “send” on the e-mail, stop for a minute and think: is everything easily understandable (with sufficient context, explanation of terms, summarization of key points, etc.) for the reader given their limited knowledge? Is it clear to the reader what the next step s/he should take as a result of the e-mail? How would the reader feel after reading? This doesn’t just apply to e-mails or Slack messages. E.g. naming your dashboard is also a form of communication.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manage expectations.&lt;/strong&gt; All the time, people end up being disappointed/frustrated or lose trust due to misaligned expectations. Most of this is preventable. Ask yourself regularly: are expectations aligned? E.g. is there a difference between your understanding of the expected timing/quality/importance/amount of your work output and a stakeholder’s understanding? If the result will be highly uncertain (as is the case for e.g. using a new machine learning model for the first time), take extra care to ensure people understand that uncertainty and don’t have unfairly high expectations. This is not just about making people happy; it often hurts a team’s effectiveness if someone is deciding a course of action based on unrealistic expectations of a colleague’s work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Err on the side of over-communication.&lt;/strong&gt; If you’re not sure if something is worth saying/communicating, it’s better to say it just in case. The potential damage from under-communication (loss of trust, major bug, etc.) is typically far greater.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proactively give both positive and negative (critical) feedback.&lt;/strong&gt; No one is perfect, no one is completely self-aware, and no one can optimize their work perfectly just by themselves. The most effective teams have members that help each other become better team members over time.&lt;br&gt;
&lt;strong&gt;Giving critical feedback:&lt;/strong&gt; keep the message clear (don’t overly sugar-coat it) while not sounding like an asshole. Provide justification and explanation about how to improve (e.g. don’t just say “your presentation sucked”).&lt;br&gt;
&lt;strong&gt;Receiving critical feedback:&lt;/strong&gt; some people suck at giving feedback. Try not to take it personally and try to turn it into something helpful. Try to give feedback on how they can give you feedback more effectively. Be very welcoming of any critical feedback and proactively invite people to give it to you.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Principles / best practices for data science and technical work&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Good technical work usually requires good communication.&lt;/strong&gt; Some people see coding etc. as a hard skill and communication as a soft skill, and some people pride themselves mainly on their hard skills. But if your code is hard to read or if your technical work is hard to follow/understand, I don’t consider that good work, especially in the context of an organization. Making your work reasonably easy to read/understand requires good communication skills.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Think carefully about the tradeoff between work speed and technical debt.&lt;/strong&gt; Sometimes it makes sense to just make a quick and dirty hacky prototype, sometimes it doesn’t. Adjust based on the situation (e.g. ask yourself questions like: will this eventually be used in production? How many people will be reading/using this code, for what purpose?)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identify and make the obviously good tradeoffs first.&lt;/strong&gt; For example, take the time to design well the folder/file structure and naming conventions for a big project. This might only take 1% of the total project time and reduce the technical debt burden by 20%. It’s surprising how many people fail to make such tradeoffs. Reflect regularly about if you’re making the right tradeoffs. Get feedback from others about it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“&lt;/strong&gt;&lt;a href="https://en.wikiquote.org/wiki/Donald_Knuth"&gt;&lt;strong&gt;Premature optimization is the root of all evil&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.”&lt;/strong&gt; True… and insufficient or overly late optimization is bad as well. One needs to think about when and how much optimization is appropriate. E.g. one rule of thumb is the &lt;a href="https://en.wikipedia.org/wiki/Rule_of_three_(computer_programming)"&gt;Rule of Three&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Work should be reproducible and understandable.&lt;/strong&gt; If you did a piece of analysis or created an ML model, someone else with sufficient technical chops should be able to come and reproduce what you did with little to no guidance from you. Otherwise, 2 problems will get worse: technical debt and over-dependency on specific individuals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Almost all code should be pushed to git&lt;/strong&gt; (or another code version control system). There are tons of benefits: reproducibility, accountability, reversibility, searchability, education of team members, etc. Exceptions might be very short, temporary scripts. If it’s too troublesome for you to push your code to git, you need to set things up so that it’s easier to do so (e.g. bash script to reduce the amount of typing when pushing to git, set up auto-sync from your Jupyter server to git, etc.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have a culture of knowledge sharing,&lt;/strong&gt; especially on technical topics. Internal/external blog posts, tech talks, tutorials, recommendations for tools to use, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Appreciate the balance between data-driven and non-data-driven methods.&lt;/strong&gt; Sometimes, companies just have to take a risk and act on gut feel . Otherwise, certain risky/innovative things will never be attempted. Often, human decision-making is a combination of data and gut feel or qualitative info, and some of the best software is a combination of ML (based on training data) and rule-based heuristics (based on human expertise).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand on the shoulders of giants&lt;/strong&gt;. No need to develop something from scratch when you can reuse something off of github.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Set up your technical environment to for efficient work&lt;/strong&gt;. Sometimes I can find the exact line of code I’m looking for in 5 seconds. Set up &lt;a href="https://winaero.com/blog/add-custom-chrome-searches-for-address-bar-to-save-your-time/"&gt;Chrome search aliases&lt;/a&gt; for github, google docs, or whatever else you use commonly. Allow efficient use of your command line using bash aliases, &lt;a href="https://ohmyz.sh/"&gt;oh-my-zsh&lt;/a&gt;, etc. Enable &lt;a href="https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/codefolding/readme.html"&gt;code-folding in Jupyter&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Block off a bit of time for innovative, cutting-edge (and hopefully fun) data science experimentation.&lt;/strong&gt; For example, check out a new paper or new Python library and see how it can be applied to problems you’re trying to solve. A common problem in organizations is that, due to time constraints and short-term goals, people are stuck with using the same old approaches/tools repeatedly (which they can implement quickly with relatively high certainty about the results). Data science / ML is a rapidly progressing field, and you will eventually be left behind or get bored if you don’t take time to try new things.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Jerry’s personality and quirks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Once I took a personality test and the automatically generated test result was that I’m a &lt;strong&gt;“&lt;/strong&gt;bull in a china shop”. I hope it’s an overstatement..&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I speak my mind&lt;/strong&gt;, sometimes bluntly. If I think something should be changed for the greater good, I may bang the table (figuratively) and argue aggressively about why it should be changed, regardless of the seniority of the people involved. Feel free to tell me to shut up (but please justify why you think I should). I think over time I’ve become more diplomatic, but this is still part of my fundamental personality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;I have grit&lt;/strong&gt;. I can deal with huge setbacks/failures, huge problems, huge challenges, etc. But I won’t sacrifice my health dealing with such problems (I did before and it’s not worth it).&lt;/li&gt;
&lt;li&gt;I have little tolerance for bullshit. Sometimes I’ll fight it patiently if I have to. (Of course, one should take care to not prematurely conclude that something is bullshit.)&lt;/li&gt;
&lt;li&gt;I don’t get embarrassed easily. Some might describe this as &lt;a href="https://www.youtube.com/watch?v=8PxEFti645A"&gt;being shameless&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I’m addicted to self-improvement and learning. Let me know if you have tips :) I still have a long way to go.&lt;/li&gt;
&lt;li&gt;I try to think more about how to do things for the mid/long-term benefit of the organization/ecosystem, not the short-term benefit of any given individual or team. If I’m not doing this, please let me know.&lt;/li&gt;
&lt;li&gt;I like &lt;strong&gt;sharing news articles, reports&lt;/strong&gt;, papers, github repos, etc. that I think might be helpful or interesting. Let me know if it’s too much or too distracting. Don’t let it hurt your ability to focus.&lt;/li&gt;
&lt;li&gt;I’m a big fan of &lt;strong&gt;Finnish values&lt;/strong&gt; (having worked in Finland for a year). Many Finnish people are humble, blunt, straightforward, concise, and have grit.&lt;/li&gt;
&lt;li&gt;I’m like the &lt;strong&gt;grammar police&lt;/strong&gt;. I will point out people’s incorrect grammar/spelling/wording, (in any language that I speak). I do it to help people improve themselves and to facilitate better communication. But if this becomes too annoying, please let me know.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Logistical stuff&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Go ahead and put things in my calendar or reschedule stuff as needed. No need to ask me in advance. My calendar is up to date.&lt;/li&gt;
&lt;li&gt;I may block off a chunk of “focused work time” on my calendar if I feel like I’m being distracted too much.&lt;/li&gt;
&lt;li&gt;Feel free to grab extra time with me if the regular 1-on-1 or team meeting is insufficient.&lt;/li&gt;
&lt;li&gt;Weekly team meetings — I think people should speak up only if one has something to say that others should hear; no need to feel pressured to always give an update on your work. Also, I usually don’t like overly structuring meetings with inflexible agendas, but we can add more structure if needed.&lt;/li&gt;
&lt;li&gt;Feel free to slack/e-mail me at any time on any day. However, I may stop checking slack/e-mail when trying to disconnect from work (on weekends etc.) in which case the best way to reach me for an emergency is via phone or Facebook Messenger.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Relevant posts by me&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://medium.com/@peacej2/how-to-be-an-awesome-data-science-manager-1c63b059a03c"&gt;How to be an awesome data science manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@peacej2/how-to-be-an-awesome-data-scientist-930f04760f7e"&gt;How to be an awesome data scientist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide was partially inspired by the &lt;a href="http://growth.eladgil.com/book/the-role-of-the-ceo/insights-working-with-claire/"&gt;Guide to Working with Claire&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/0*H13cLJi5Z5CThoew.jpg"&gt;Typical picture of me working with a colleague&lt;/p&gt;</content><category term="Working as a data scientist"></category><category term="data science"></category><category term="communication"></category><category term="teamwork"></category></entry><entry><title>How To Be An Awesome Data Science Manager</title><link href="https://jerrychi.com/how-to-be-an-awesome-data-science-manager.html" rel="alternate"></link><published>2019-03-21T00:00:00+09:00</published><updated>2019-03-21T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-03-21:/how-to-be-an-awesome-data-science-manager.html</id><summary type="html">&lt;p&gt;9 min read&lt;/p&gt;
&lt;p&gt;This is a relatively new type of job, and many organizations and people are still in the midst of figuring out what makes a great data science manager.&lt;/p&gt;
&lt;p&gt;What does it even mean to be “awesome” in this context? Well, if your team members…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;…are highly motivated …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;9 min read&lt;/p&gt;
&lt;p&gt;This is a relatively new type of job, and many organizations and people are still in the midst of figuring out what makes a great data science manager.&lt;/p&gt;
&lt;p&gt;What does it even mean to be “awesome” in this context? Well, if your team members…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;…are highly motivated&lt;/li&gt;
&lt;li&gt;…are growing quickly&lt;/li&gt;
&lt;li&gt;…have great impact on your organization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;then you are probably an awesome data science manager.&lt;/p&gt;
&lt;p&gt;Here’s my two cents on how to be awesome:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Be an &lt;a href="https://medium.com/@peacej2/how-to-be-an-awesome-data-scientist-930f04760f7e"&gt;awesome data scientist&lt;/a&gt; yourself&lt;/li&gt;
&lt;li&gt;Enable and empower your team members&lt;/li&gt;
&lt;li&gt;Don’t just prioritize; show them how to prioritize&lt;/li&gt;
&lt;li&gt;Value their future careers&lt;/li&gt;
&lt;li&gt;Maximize synergies with other teams&lt;/li&gt;
&lt;li&gt;Hire more awesome data talent&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;1 Be an awesome data scientist yourself&lt;/h1&gt;
&lt;p&gt;========================================&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://medium.com/@peacej2/how-to-be-an-awesome-data-scientist-930f04760f7e"&gt;my last post on this topic&lt;/a&gt;. It helps a lot if you can provide ideas and advice on deciding between analytical approaches, improving code quality, choosing which frameworks/tools to use for a machine learning project, etc. You need to stay up to speed on data science skills and knowledge by being hands-on at least part of the time, even if you manage a lot of people.&lt;/p&gt;
&lt;p&gt;If you know that you’re an awesome data scientist, &lt;strong&gt;be both humble and confident&lt;/strong&gt; about it. Be humble because there is still so much room to improve and learn. But you should also radiate confidence so that your team members will naturally want to learn from you, and so that other teams will take your team seriously.&lt;/p&gt;
&lt;h1&gt;2 Enable and empower your team members&lt;/h1&gt;
&lt;p&gt;=======================================&lt;/p&gt;
&lt;p&gt;At one of the management culture spectrum, you have the “slave driver” managers, figuratively whipping their team members forced to do specific tasks. At the other end, you have managers focused on &lt;strong&gt;empowering their team members with the freedom and ability to make big decisions and do big things&lt;/strong&gt;. (An extreme example of this is &lt;a href="https://www.gamesindustry.biz/articles/2018-05-04-supercell-meet-the-worlds-least-powerful-ceo"&gt;Supercell&lt;/a&gt;, a past employer of mine.)&lt;/p&gt;
&lt;p&gt;This doesn’t mean you simply throw people into the deep end of the pool and walk away. Rather, you should step in to offer help or guidance as needed. The appropriate frequency of such help will probably differ by team member, given their abilities/experience and the task at hand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enablement:&lt;/strong&gt; give them the tools, data, and help that they need and get obstacles out of their way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;someone need to get internal approval to access a dataset? B̵a̵n̵g̵ ̵t̵h̵e̵ ̵t̵a̵b̵l̵e̵ proactively explain to the approver to get it approved quickly&lt;/li&gt;
&lt;li&gt;someone wants to get a second monitor? a paid license for RStudio? company doesn’t want to pay for it?B̵a̵n̵g̵ ̵t̵h̵e̵ ̵t̵a̵b̵l̵e̵ ̵h̵a̵r̵d̵e̵r̵ ̵a̵n̵d̵ ̵r̵i̵o̵t̵ actively lobby your company for these basic benefits to increase productivity&lt;/li&gt;
&lt;li&gt;someone struggling with building a data pipeline by him/herself? You could provide some tips yourself or connect the struggling person to someone from another team (e.g. data engineering team) who has more expertise in such pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Empowerment:&lt;/strong&gt; give and trust them with the authority, independence, freedom, and abilities that they need to kick ass.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Allow them to &lt;strong&gt;take full ownership&lt;/strong&gt; on some projects or areas of work. Trust them by default; i.e. trust that they will do a good job without micromanagement. For less experienced team members, you can start with smaller projects first to minimize the risk. If you found that you over-trusted someone (and they screwed up), forgive them and help them about a bit more frequently the next time. I believe that in most cases, the risk/damage from under-trusting people is in general much greater than the risk/damage from over-trusting people. Assuming you’re working with good people, they’ll do their best to step up to challenges that you entrust them with, and they’ll feel motivated and grow quickly while doing so.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Give them freedom.&lt;/strong&gt; People are the most motivated when they are free to choose what to do; I think this is especially true for data science. Of course, you can’t just let them spend all their time iterating on convolutional neural networks for classifying hot dogs vs. non-hot dogs… you also need to think about your team’s impact on the company, so you need to be constantly solving the dual-objective optimization problem of allowing people to freely choose the work they want to do with while maximizing the impact of that work on the company. Taking the time to explain to your team members regarding the interestingness and importance of certain projects to the company can help you reach a higher global optimum.&lt;/li&gt;
&lt;li&gt;Help them &lt;strong&gt;gain abilities and skill up&lt;/strong&gt; in ways that allow them to do bigger things at your company. This could be a full separate essay, I’ll skip the details for now.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;3 Don’t just prioritize; show team members how to prioritize&lt;/h1&gt;
&lt;p&gt;=============================================================&lt;/p&gt;
&lt;p&gt;Prioritization for data scientists can be complex and difficult. In fact it’s even sometimes a meta-problem; since often the output of data science work could be advice on whether or not to prioritize a product feature, you might need to think about prioritizing the work on understanding priority…&lt;/p&gt;
&lt;p&gt;Typically, you’ll be in a better position to prioritize things at a high level than your team members (given that you have more experience, you’re more in sync with company executives and their goals, you have more broad understanding of the various current and future projects, etc.). But that doesn’t mean you should just decide the task priorities and assign them to your team members.&lt;/p&gt;
&lt;p&gt;In addition to deciding the priorities, try to &lt;strong&gt;ensure deep understanding amongst your team of &lt;em&gt;why&lt;/em&gt; a given task has a certain level of importance and potential impact&lt;/strong&gt;. The benefits of doing this are threefold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Team members will likely &lt;strong&gt;become better at triaging by themselves&lt;/strong&gt; further subtasks or new requests (e.g. dashboard creation request) of a project; this is especially useful if they are working on multiple projects simultaneously&lt;/li&gt;
&lt;li&gt;They’ll naturally &lt;strong&gt;gain better prioritization skills over time&lt;/strong&gt; (which is good for many things including the possibility of future promotion to be a data science manager)&lt;/li&gt;
&lt;li&gt;They’ll likely &lt;strong&gt;feel more motivated&lt;/strong&gt; about doing the work (even if they didn’t exactly choose to do the work)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;4 Value team members’ future careers&lt;/h1&gt;
&lt;p&gt;=====================================&lt;/p&gt;
&lt;p&gt;As an awesome manager, you should care not just about the current performance of your team members… if you really care about them as human beings, you would care about their long-term success and well-being as well. In your position as a manager, you can actually contribute a lot to their future careers.&lt;/p&gt;
&lt;p&gt;Data science career planning is especially tough because the occupation is relatively new, things are changing fast, and your team members might only have short experience as a data scientist.&lt;/p&gt;
&lt;p&gt;Here are things to try:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Set the cultural norm&lt;/strong&gt; that it is OK (but not required) to freely talk about career aspirations, including potentially leaving your team. Even if the person leaves your team, you could still end up working together on cross-team projects, and the person could be leveraging their data science skills in their next role (e.g. as a PM making data-driven product decisions or another type of data role).&lt;/li&gt;
&lt;li&gt;Have periodic &lt;strong&gt;1-on-1 career chats,&lt;/strong&gt; maybe every 3~6 months. Ask probing questions (without being creepy :P ) to make them think more deeply about the kind of work they enjoy and are passionate about.&lt;/li&gt;
&lt;li&gt;Be on the lookout for &lt;strong&gt;chances for them to try out the work&lt;/strong&gt; that they aspire to do in the future. For example, if someone aspires to be a data science manager in the future, you could set her up as the official mentor to the new summer intern, since mentoring is part of the work of a manager. If someone aspires to become an expert on hardcore ML engineering in the future rather than ad hoc analytics, try to arrange for him to get assigned to more ML projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;5 Maximize synergies with other teams&lt;/h1&gt;
&lt;p&gt;======================================&lt;/p&gt;
&lt;p&gt;Data science work is usually not valuable in and of itself. It needs to somehow fit well into a broader product, project, or organization to be truly meaningful. I’ve seen many examples of this not working well; e.g. a great dashboard or machine learning model was made but ends up having no impact. Furthermore, many people are not used to working with data scientists, so achieving smooth teamwork may be difficult.&lt;/p&gt;
&lt;p&gt;Here are some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In general, &lt;strong&gt;manage and shape expectations&lt;/strong&gt; of what your team can provide and how people should interact with your team. For example, you can tell other teams: “don’t just dump a random analysis request on us without explaining the context. We’ll only agree to do the work if we understand the importance.” You can even make a slide deck to the tune of “this is how the data science team functions and here is the best way to work with us,” then slot in some time to present it. Shaping expectations is also important to prevent demotivation of your team members caused by e.g. unreasonable requests.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Go with the flow.&lt;/strong&gt; In other words, try to deeply understand the high-level company direction, goals, and priorities, as well as how executives are thinking about them. If you ideate projects in a way that is aligned with those goals, it will be easier to convince other teams to collaborate with your team on those projects. Of course, if you believe the company direction is flawed, you should leverage your mad data skills to change that direction… but that’s another topic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clarify the inter-team dependencies&lt;/strong&gt; for a project to succeed, then talk to those other teams to get their understanding and commitment. Do this &lt;strong&gt;early on&lt;/strong&gt;, i.e. before any major work is started. As a contrived simplified example, let’s say your team member wants to create a customer lifetime value prediction model to help marketers optimize online ad spend. In order for the project to succeed, 1. the data engineering team needs to ensure the data pipeline (for the model’s input data) is stable with a proper SLA and understanding of its importance 2. The marketing team needs to agree to integrate the model output into their daily online ad management workflow somehow. If you can’t get at least some degree of commitment early on, it might be better to give up or postpone the project.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;6 Hire more awesome data talent&lt;/h1&gt;
&lt;p&gt;================================&lt;/p&gt;
&lt;p&gt;Most organizations don’t have enough data talent to unlock their maximum potential. Some companies just declare “let’s set a goal to hire 50 AI engineers” and have the flawed expectation that that will automagically lead to awesomeness.&lt;/p&gt;
&lt;p&gt;How can you help your organization? It’s not just about your specific team. Not only does your team have dependencies on other teams (like data engineering), you want to help &lt;strong&gt;drive your whole organization to data greatness&lt;/strong&gt;, right?&lt;/p&gt;
&lt;p&gt;Here are some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assess and explain the needs for various types of data roles&lt;/strong&gt; and how they should fit into your organization. Convince your company to either start hiring or increase priority of hiring for some of these roles. It might be initially hard to convince executives, so try to provide concrete examples of the work such people would be doing at your organization specifically. You can refer to how some top tech companies define their data roles (see posts by &lt;a href="https://www.linkedin.com/pulse/one-data-science-job-doesnt-fit-all-elena-grewal/"&gt;AirBnB&lt;/a&gt; and &lt;a href="https://medium.com/indeed-engineering/theres-no-such-thing-as-a-data-scientist-8dae923c14e3"&gt;Indeed&lt;/a&gt;; &lt;a href="https://medium.com/swlh/building-a-world-class-growth-team-25cb508e666d"&gt;post on org structure of growth teams&lt;/a&gt;; &lt;a href="https://www.quora.com/What-does-a-data-product-manager-do"&gt;Quora page about Data Product Managers&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;For your data science team specifically, think hard about 1. how to &lt;strong&gt;bridge the gap&lt;/strong&gt; between current/future company needs and your current team and 2. how &lt;strong&gt;team members complement each other,&lt;/strong&gt; then try to prioritize hiring accordingly. For example, if your team is full of people good at software engineering but bad at data visualization/communication, you might want to find someone better at communication, even if he is a bit weaker at coding. In general, it’s very hard to find talent with all desirable traits/skills of a data scientist, so you need to rely on team members complementing each other, both for making the team overall more well-rounded and allowing people to learn from each other and help each other grow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Much more can be said about the topic of data talent (including how to attract and evaluate such talent), but maybe that can be a separate post.&lt;/p&gt;
&lt;h1&gt;Closing&lt;/h1&gt;
&lt;p&gt;Being a data science manager is tough and challenging, but it’s a very rewarding job. I hope this post provided some helpful food for thought. I’m still learning myself, so I would appreciate any thoughts/comments/advice below. Thanks~ (^ㅂ^)&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*IddD2FuMkk7fuRqByBiotQ.jpeg"&gt;Obligatory random abstract image&lt;/p&gt;</content><category term="Working as a data scientist"></category><category term="data science"></category><category term="communication"></category></entry><entry><title>How To Be An Awesome Data Scientist</title><link href="https://jerrychi.com/how-to-be-an-awesome-data-scientist.html" rel="alternate"></link><published>2019-02-16T00:00:00+09:00</published><updated>2019-02-16T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-02-16:/how-to-be-an-awesome-data-scientist.html</id><summary type="html">&lt;p&gt;·7 min read&lt;/p&gt;
&lt;p&gt;The world is teeming with people who have the “data scientist” title but somehow don’t live up to expectations. This problem might be related to the use of the term “fake data scientist,” although I don’t think we should be snobby about academic qualifications etc …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·7 min read&lt;/p&gt;
&lt;p&gt;The world is teeming with people who have the “data scientist” title but somehow don’t live up to expectations. This problem might be related to the use of the term “fake data scientist,” although I don’t think we should be snobby about academic qualifications etc.&lt;/p&gt;
&lt;p&gt;Here is my humble opinion on &lt;strong&gt;how to be an awesome data scientist, regardless of your background&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Always be learning useful things efficiently&lt;/li&gt;
&lt;li&gt;Proactively challenge yourself&lt;/li&gt;
&lt;li&gt;Focus on impact early on&lt;/li&gt;
&lt;li&gt;Have pride in both technical skills and business understanding&lt;/li&gt;
&lt;li&gt;Communication: think hard about how to convey insights and ideas&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Tip #1: Always be learning useful things efficiently&lt;/h1&gt;
&lt;p&gt;The world of data science is progressing quickly.. you’ll fall behind quickly if you stop learning.&lt;/p&gt;
&lt;p&gt;Note how I &lt;strong&gt;didn’t&lt;/strong&gt; simply say “always be learning.” That is not enough. I believe that most people, when learning, spend most of their time non-optimally. An extreme example of this is trying to learn English by reading a dictionary from cover to cover (I actually met someone who had been doing this for years and was strangely proud of knowing all words that start with “A” …)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learn useful things.&lt;/strong&gt; Considering a new programming language? a new library for creating charts? A new machine learning framework? Before starting, carefully estimate the long-term usefulness of acquiring the skill. For example, popularity is one thing to consider. You can guess that Tensorflow will be a useful skill to have even just by checking simple metrics such as the trend of Github repo stars and Google searches for “Tensorflow.” You can also look at &lt;a href="https://www.jetbrains.com/research/python-developers-survey-2018/"&gt;surveys of programmers&lt;/a&gt; / data scientists, &lt;a href="https://www.quora.com/What-are-the-skills-required-to-become-a-Data-Scientist"&gt;find or ask questions on Quora&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn things efficiently.&lt;/strong&gt; Think of 100 students sitting in a classroom. The lecture may be too easy / slow for 70 of the students, too difficult / fast for 25 of the students, and just right for 5 of the students. You can customize the learning difficulty and speed/frequency in a way that’s optimal for yourself. (E.g. watch MOOC lecture videos at 1.5x speed, skip through materials in books that you already know, etc.) When learning, you should constantly feel slightly/somewhat challenged and forced to think, but not overwhelmed. Also, refer to study hacks for efficiency such as &lt;a href="https://medium.freecodecamp.org/why-i-studied-full-time-for-8-months-for-a-google-interview-cc662ce9bb13/#f8bf"&gt;spaced repetition&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, sometimes it’s fun to learn something obscure or useless… just because it’s interesting. That’s great for having a fun life. But this article is about being an awesome data scientist :)&lt;/p&gt;
&lt;h1&gt;Tip #2: Proactively challenge yourself&lt;/h1&gt;
&lt;p&gt;This actually overlaps with tip #1, but it’s important to emphasize.&lt;/p&gt;
&lt;p&gt;It’s easy to fall into a cycle of doing similar things over and over at work — things which feel less and less challenging.&lt;/p&gt;
&lt;p&gt;no challenge → no growth&lt;/p&gt;
&lt;p&gt;Building a bar chart of revenue for the 50th time? Determining statistical significance of an A/B test of button colors yet again? If you go through a full week in which everything you did was “been there, done that”… &lt;strong&gt;you need to change something&lt;/strong&gt;. There are many ways you can do so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Try a different (hopefully better) way of doing things&lt;/strong&gt;. Can you automate more of your work or produce a better result? Instead of typing the same date filter repeatedly in 4 subqueries of your SQL query, why not instead develop a way to templatize the SQL query so that you only have to type the date once? Instead of static image charts, can you generate interactive charts using a different R/Python library? Of course, you should probably refrain from things that are overkill (e.g. using a deep neural network when simple logistic regression already works very well).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ask or volunteer for more or different responsibilities&lt;/strong&gt;. This could be a project that is ideated and proposed by you. For example, you could ask to work on a different product area, or you could volunteer to create an automated A/B test analysis tool. You could even volunteer to give a tech talk to practice your public speaking skills, even if your company doesn’t have a culture of holding tech talks. You could also ask your colleagues/boss for ideas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change teams (or even companies)&lt;/strong&gt;. Hey, no hard feelings. If you’re not able to find enough challenges in your current team despite trying hard to do so, it’s time to move on. There are tons of interesting opportunities out there. You can still keep in touch with your former teammates and grab a drink with them occasionally.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Tip #3: Focus on impact early on&lt;/h1&gt;
&lt;p&gt;It’s common for great work to be done but not have an &lt;strong&gt;impact&lt;/strong&gt;. This can lead to demotivation and lack of promotions. Some typical scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You showed in a Jupyter notebook that you can develop a model to predict user churn with high accuracy; however, the model was never deployed to production&lt;/li&gt;
&lt;li&gt;Some dude asks you to pull some data. You pull the data. The dude says “ah, interesting...” End of story.&lt;/li&gt;
&lt;li&gt;You estimated that an additional 50,000 users could be acquired with no extra budget if the marketing team changes how they allocate TV ad budget to time slots. The ultimate decision maker says the allocation has “already been finalized.”&lt;/li&gt;
&lt;li&gt;If a tree falls in the forest and no one is around to hear it, does it make a sound? If you created a dashboard and no one looks at it….&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To prevent situations like the above, before you commit to doing any major piece of work, ask yourself:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What &lt;strong&gt;ultimate impact on the company&lt;/strong&gt; do you expect from doing this piece of work? Is the impact-to-work ratio higher than other potential tasks?&lt;/li&gt;
&lt;li&gt;What are the &lt;strong&gt;mechanisms/requirements&lt;/strong&gt; for enabling the impact? Often this involves checking for &lt;strong&gt;actionability&lt;/strong&gt;, i.e. if a human needs to take action to enable the impact, get that human’s agreement before you start the work. I.e. ask them “if my analysis indicates X, then based on that, will you probably do Y?”&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Tip #4: Have pride in both technical skills and business understanding&lt;/h1&gt;
&lt;p&gt;I think almost all data scientists understand that technical skills are important: programming, statistics, cloud computing, etc. But sometimes I feel there is not enough appreciation for &lt;strong&gt;deeply understanding the product or business&lt;/strong&gt; for which data science is being applied&lt;strong&gt;.&lt;/strong&gt; Things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the strategic positioning of your product on your market vs. competitors?&lt;/li&gt;
&lt;li&gt;What is the thought process of customers (who are humans, not just &lt;code&gt;user_id&lt;/code&gt;s) when they decide to spend money on your product?&lt;/li&gt;
&lt;li&gt;How does the overall ecosystem work? For example, if your product is an online news aggregator, how do advertisers, publishers, tech platforms, and users depend on and affect each other? How do the power dynamics work? For a good example of someone who really understands tech ecosystems, see &lt;a href="https://www.ben-evans.com/archive"&gt;Ben Evan’s newsletter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It’s easy to get bogged down just looking at data, which are in a way abstractions of your customers, partners, etc. But to really have a big impact, you often have to tie the data to the real world in the right way. That’s hard to do, but it’s much easier if you have the right understanding.&lt;/p&gt;
&lt;p&gt;I’ve seen many resumes that are just a laundry list of data science techniques and tools which have almost no mention of the product/business implications of their work. This tells me the person doesn’t fully appreciate such implications.&lt;/p&gt;
&lt;h1&gt;Tip #5: Communication: think hard about how to convey insights and ideas&lt;/h1&gt;
&lt;p&gt;I considered simply writing “communicate better,” but that’s a no-brainer. Who wouldn’t want to be a better communicator? Communication is a natural challenge for many people, but I think huge improvement can be made by simply &lt;strong&gt;taking a bit of time to think&lt;/strong&gt; about it.&lt;/p&gt;
&lt;p&gt;If you’re not sure where to start, you can start with improving your written communication — which is easier since you have more time to think — and then move on to improving verbal communication. Data scientists’ communication includes &lt;a href="http://www.storytellingwithdata.com/"&gt;not only words but also charts and diagrams&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many people don’t really &lt;strong&gt;reflect&lt;/strong&gt; much on their own communication. They don’t stop to think before they type a message on Slack or take another look aftering posting the message to make sure it conveys their point clearly.&lt;/p&gt;
&lt;p&gt;lack of reflection → lack of improvement&lt;/p&gt;
&lt;p&gt;Improving communication could be as simple as asking yourself a checklist of questions such as this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Did you write things in a logical order? You could write things in order of importance or in chronological order.&lt;/li&gt;
&lt;li&gt;Who are the intended audiences? Is your message / e-mail / blog post customized for such audiences? Will the audience understand the main point you’re trying to make within 10 seconds of starting to read? (this is especially important if your audience is busy executives)&lt;/li&gt;
&lt;li&gt;Are you using too much technical jargon or acronyms which some people might not understand?&lt;/li&gt;
&lt;li&gt;Is your message ambiguous? This is especially a common problem when talking about metrics. Is it insufficient to simply say “revenue” if there are multiple definitions of revenue? If you made a chart, is it easy to interpret and get the key takeaway from it?&lt;/li&gt;
&lt;li&gt;Should you highlight or &lt;strong&gt;put in bold&lt;/strong&gt; especially important points? (especially useful if there’s a lot of text)&lt;/li&gt;
&lt;li&gt;Instead of a wall of text, should you use a diagram to summarize the situation?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also proactively ask others for feedback on your communication. I’d also like to welcome you to give feedback to me on this Medium post :)&lt;/p&gt;
&lt;p&gt;After you get better at communication via deliberate thinking, it’ll become more natural and automatic for you.&lt;/p&gt;
&lt;h1&gt;Up next&lt;/h1&gt;
&lt;p&gt;[Update 2019 Mar 19] I wrote a post called “&lt;a href="https://medium.com/@peacej2/how-to-be-an-awesome-data-science-manager-1c63b059a03c"&gt;How to be an awesome data science manager&lt;/a&gt;.”&lt;/p&gt;</content><category term="Working as a data scientist"></category><category term="data science"></category><category term="communication"></category><category term="teamwork"></category></entry><entry><title>Generating Fake K-pop Faces Part 1</title><link href="https://jerrychi.com/generating-fake-k-pop-faces-part-1.html" rel="alternate"></link><published>2019-02-11T00:00:00+09:00</published><updated>2019-02-11T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-02-11:/generating-fake-k-pop-faces-part-1.html</id><summary type="html">&lt;p&gt;·5 min read&lt;/p&gt;
&lt;p&gt;This project is for my own personal fun and learning. Hopefully it’ll be fun for some to read about my progress as well.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*b7Yy0V2cHo5n2yIZxlJMMw.gif"&gt;My first (poor) attempt to generate faces. Read on to see how I did this.&lt;/p&gt;
&lt;h1&gt;Planned approach&lt;/h1&gt;
&lt;p&gt;I expect this project to have …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·5 min read&lt;/p&gt;
&lt;p&gt;This project is for my own personal fun and learning. Hopefully it’ll be fun for some to read about my progress as well.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*b7Yy0V2cHo5n2yIZxlJMMw.gif"&gt;My first (poor) attempt to generate faces. Read on to see how I did this.&lt;/p&gt;
&lt;h1&gt;Planned approach&lt;/h1&gt;
&lt;p&gt;I expect this project to have maybe 7 steps. I’ve done steps 1 and 2.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;gather, filter, and normalize training data (images)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a simple autoencoder (neural network)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a variational autoencoder (VAE)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a vanilla generative adversarial network (GAN)&lt;/li&gt;
&lt;li&gt;generate female K-pop faces using a &lt;a href="https://github.com/hindupuravinash/the-gan-zoo"&gt;fancier GAN architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;switch to using cloud GPUs instead of my Macbook (if I haven’t already) for better results; tune hyperparameters etc. a bit more&lt;/li&gt;
&lt;li&gt;try expanding to J-pop faces and/or male faces? (thanks to &lt;a href="https://medium.com/@daisukeishii"&gt;Daisuke Ishii&lt;/a&gt; of Team AI for the suggestion)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Step 1: gather, filter, and normalize images&lt;/h1&gt;
&lt;p&gt;This was actually a bit tricky, and I wanted to do it without any manual human filtering (because 1. it was a fun technical challenge for myself and 2. I wanted the process to be easily scalable to more images or other categories of people)&lt;/p&gt;
&lt;h2&gt;Downloading images&lt;/h2&gt;
&lt;p&gt;It was surprisingly easy to crawl/scrape images from Google, Bing, and Baidu image search given the Python package &lt;a href="https://pypi.org/project/icrawler/"&gt;icrawler&lt;/a&gt;. Here is a code snippet showing how easy this is.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;keywords = \[&amp;#39;걸그룹 프로필 사진&amp;#39;, &amp;#39;걸그룹 셀카&amp;#39;\] # e.g. &amp;quot;girl group profile pic&amp;quot;**for** keyword **in** keywords:  
    google\_crawler.crawl(keyword=keyword, max\_num=300, min\_size=(300,300), file\_idx\_offset=&amp;#39;auto&amp;#39;)  
    bing\_crawler.crawl(keyword=keyword, max\_num=100, min\_size=(300,300), file\_idx\_offset=&amp;#39;auto&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(full code &lt;a href="https://github.com/peacej/CADL/blob/master/kpop-faces/scrape_kpop_images.ipynb"&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I used multiple search engines and keyword phrases in different languages both to get more input data (images) and provide more variety in the input data (variety deriving both from algorithmic differences and cultural differences, i.e. Chinese journalists might depict K-pop a bit differently). Another reason for using Chinese keywords for Baidu image search: it seems that Baidu tends to do a poor job of providing relevant search results when searching in other languages.&lt;/p&gt;
&lt;h2&gt;Normalize/preprocess images&lt;/h2&gt;
&lt;p&gt;I won’t get into the details, but normalization involved mainly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;object localization and cropping (i.e. find the bounding box for a face in an image and crop to that box; sometimes this involved outputs of multiple face images per raw input image, since multiple people can be in one input image)&lt;/li&gt;
&lt;li&gt;resizing&lt;/li&gt;
&lt;li&gt;alignment (sometimes a face is tilted diagonally; I rotated the face so that eyes are perfectly horizontal)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I did all of the above by adapting existing code snippets and using the &lt;a href="https://opencv.org/"&gt;OpenCV&lt;/a&gt;, &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt;, and &lt;code&gt;[imutils](https://github.com/jrosebr1/imutils)&lt;/code&gt; packages.&lt;/p&gt;
&lt;h2&gt;Filter images&lt;/h2&gt;
&lt;p&gt;For the time being, I wanted to ensure that the input data includes only female faces, not male. Since gender classification is already a common ML task, I thought I would re-use an existing model. In fact, I ended up using TWO existing models (&lt;a href="https://modeldepot.io/harshsikka/gender-classification"&gt;1&lt;/a&gt;, &lt;a href="https://github.com/oarriaga/face_classification/blob/master/trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5"&gt;2&lt;/a&gt;) with different neural network architectures and only let images through if both models classified them as “female” above a certain probability threshold.&lt;/p&gt;
&lt;p&gt;One might point out that if I just re-use an existing gender classification model without understanding it, I might be unknowingly introducing bias. For example, if the model had been trained mainly on caucasian faces, then the model might only be able to output high probabilities of “female” when the face looked somewhat similar to a caucasian face (and the model might be less certain otherwise). Well, that’s OK for now, because 1. I’m just having fun here and 2. the result seemed to be a good representation of female K-pop faces (in terms of my own qualitative judgement).&lt;/p&gt;
&lt;p&gt;A further side benefit of using this gender classification filter is that, for images that are somehow weird or failed to be normalized properly (e.g. the the person is making a funny face or the picture is actually of an animal not a human), the gender classifier will tend to be uncertain about the gender classification, and hence such images would get filtered out by this filter.&lt;/p&gt;
&lt;p&gt;After normalization and filtering, the average image looked like the below. I was pleasantly surprised at how well the position of eyes matched across images. (In contrast, when I took the average of input images for another project years ago, the result was just a meaningless gray blob.) Seems like I did an OK job of normalizing and filtering :)&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*IC497K65IJlN4foz6DsMPg.png"&gt;The average input image&lt;/p&gt;
&lt;p&gt;Also, to more specifically visualize the variation across images (darker = less variation):&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*c7rKwRd9W2Zx7hFjHLkljg.png"&gt;Color = the mean across color channels of the standard deviation across images for each pixel&lt;/p&gt;
&lt;p&gt;Also, here’s a montage of a random subset of the input images (after normalization/filtering):&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*33bb0esPAC3JHVOU9PsZ1A.png"&gt;I’m sorry if this violates copyrights&lt;/p&gt;
&lt;h1&gt;&lt;strong&gt;Step 2: train and use a simple autoencoder&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;The autoencoder architecture I used was simple and conceptually similar to the below. Each column, a vector of values, is a layer in the neural network. We train the autoencoder by trying to make it produce an output image that is similar to each input image, which forces the “bottleneck” (middle layer) to capture the essence of the image in a very small vector of numbers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/0*H2au8b6rXfHwNSzp.png"&gt;Image stolen from &lt;a href="https://www.jeremyjordan.me/autoencoders/"&gt;jeremyjordan.me&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For now, I used only a vector of 2 values (elements) as the bottleneck in the middle. It turned out that, without any intentional design on my part, these 2 values ended up representing &lt;strong&gt;hair color&lt;/strong&gt; and &lt;strong&gt;horizontal face rotation&lt;/strong&gt;, respectively (or something close to that). I think a casual human observer would also qualitatively comment that these 2 factors (hair color and face rotation) are the primary cause of variance between images, so it’s cool that the neural network discovered the same factors automagically. This is one big use case for autoencoders (and some other ML algorithms) in general: you can boil down complex information to its essence, which can be represented simply.&lt;/p&gt;
&lt;p&gt;The animation at the top of this post combines the results from varying the 2 values in the bottleneck vector between -1 and 1 and then applying the decoder to those values.&lt;/p&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;The code is not super clean, but &lt;a href="https://github.com/peacej/CADL/tree/master/kpop-faces"&gt;here it is&lt;/a&gt;.&lt;br&gt;
A good part of the code was borrowed from &lt;a href="https://github.com/pkmital/CADL/tree/master/session-3"&gt;session 3&lt;/a&gt; of the class &lt;a href="https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info"&gt;Creative Applications of Deep Learning Using Tensorflow on the e-learning platform Kadenze&lt;/a&gt;. It’s a great class, but it might be a bit out of date.&lt;/p&gt;
&lt;h1&gt;See you next time!&lt;/h1&gt;
&lt;p&gt;Please stay tuned for part 2 =D Hopefully I’ll be able to generate more realistic images next time.&lt;/p&gt;</content><category term="AI art"></category><category term="AI art"></category><category term="tutorial"></category><category term="computer vision"></category></entry><entry><title>My Favorite Mind Blowing ML/AI Breakthroughs</title><link href="https://jerrychi.com/my-favorite-mind-blowing-mlai-breakthroughs.html" rel="alternate"></link><published>2019-02-10T00:00:00+09:00</published><updated>2019-02-10T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-02-10:/my-favorite-mind-blowing-mlai-breakthroughs.html</id><summary type="html">&lt;p&gt;·7 min read&lt;/p&gt;
&lt;p&gt;Compared to other fields, machine learning / artificial intelligence seems to have a much higher frequency of super-interesting developments these days. Things that make you say “wow” or even “what a time to be alive!” (as the creator of &lt;a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg"&gt;Two Minute Papers&lt;/a&gt; always says)&lt;/p&gt;
&lt;p&gt;Disclaimer: I’m not …&lt;/p&gt;</summary><content type="html">&lt;p&gt;·7 min read&lt;/p&gt;
&lt;p&gt;Compared to other fields, machine learning / artificial intelligence seems to have a much higher frequency of super-interesting developments these days. Things that make you say “wow” or even “what a time to be alive!” (as the creator of &lt;a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg"&gt;Two Minute Papers&lt;/a&gt; always says)&lt;/p&gt;
&lt;p&gt;Disclaimer: I’m not using any rigorous definition of “mind-blowing” or “breakthrough”; it’s a casual list.. and I might use less rigorous terminology to make this post more accessible&lt;/p&gt;
&lt;h1&gt;Amazingly accurate estimates from seemingly unusable information&lt;/h1&gt;
&lt;h2&gt;Through-wall human pose estimation&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://rfpose.csail.mit.edu/"&gt;Website/video by MIT researchers, 2018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/0*uqi7O3ByIKTsf6Ps.png"&gt;&lt;/p&gt;
&lt;p&gt;We can accurately estimate how a human on the other side of a wall is standing/sitting/walking just from perturbations in Wifi signals caused by that human.&lt;/p&gt;
&lt;h2&gt;Gauging materials’ physical properties from video&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://news.mit.edu/2015/visual-microphone-identifies-structural-defects-0521"&gt;Article/video by MIT researchers, 2015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The researchers first &lt;a href="http://news.mit.edu/2014/algorithm-recovers-speech-from-vibrations-0804"&gt;demonstrated in 2014&lt;/a&gt; that they can e.g. reproduce human speech from video (with no audio) of a potato chip bag based on the vibrations. This part was done without machine learning. In 2015, they used machine learning to show that you can estimate the stiffness, elasticity, weight per unit area, etc. of materials just from a video (in some cases just the vibrations caused by the ordinary circulation of air was sufficient).&lt;/p&gt;
&lt;h2&gt;Estimating keystrokes from a smartphone next to the keyboard&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.sigmobile.org/mobicom/2015/papers/p142-liuA.pdf"&gt;Paper, 2015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/2000/1*O9Kq4tclO75FZLiZVMkdDg.png"&gt;&lt;/p&gt;
&lt;p&gt;Researchers showed that with the audio recorded by a single, off-the-shelf smartphone placed next to a keyboard, one can estimate with &lt;strong&gt;94% accuracy&lt;/strong&gt; the individual keystrokes. Unlike previous approaches that used supervised deep learning with many microphones placed around the keyboard, this paper actually uses a relatively simple machine learning technique (K-means clustering) and &lt;strong&gt;unsupervised&lt;/strong&gt; learning.&lt;/p&gt;
&lt;h1&gt;Generative models&lt;/h1&gt;
&lt;h2&gt;Realistic face generation, style-mixing, and interpolation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1812.04948"&gt;Paper&lt;/a&gt;/&lt;a href="https://www.youtube.com/watch?v=kSLJriaOumA"&gt;video&lt;/a&gt; by NVIDIA researchers, 2018&lt;/p&gt;
&lt;p&gt;&lt;img alt="Image result for stylegan" src="https://miro.medium.com/proxy/0*eeFaGLx96mlbQcrK.gif"&gt;&lt;/p&gt;
&lt;p&gt;The researchers combined a new architecture with tons of GPUs to create extremely photo-realistic artificial faces that are interpolations between other faces or applications of the “style” of one face to another face. The work builds upon past work on Generative Adversarial Networks (GANs). GANs were invented in 2014 and have seen an explosion in research since then. The most basic concept of GANs is two neural networks dueling against each other (e.g. one that classifies images as “real” or “fake” and a second neural network that generates images in a way that attempts to “trick” the first neural network into wrongly classifying fake images as real…hence the second neural network is an “adversary” to the first).&lt;/p&gt;
&lt;p&gt;In general, there is a lot of &lt;a href="https://github.com/yenchenlin/awesome-adversarial-machine-learning"&gt;awesome research&lt;/a&gt; about adversarial machine learning, which has been around for more than a decade. There are many creepy implications for cybersecurity etc. But I digress.&lt;/p&gt;
&lt;h2&gt;Teaching machines to draw&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://ai.googleblog.com/2017/04/teaching-machines-to-draw.html"&gt;Blog post by Google Brain, 2017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/0*m_S6mdQhp1Xz8G7V.png"&gt;Interpolation between 2 drawings&lt;/p&gt;
&lt;p&gt;My acquaintance &lt;a href="https://twitter.com/hardmaru"&gt;David Ha at Google Brain&lt;/a&gt; used a generative recurrent neural network (RNN) to make drawings that are vector-based graphics (I think of this as Adobe Illustrator except automated).&lt;/p&gt;
&lt;h2&gt;Transferring great dance moves to poor dancers&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://carolineec.github.io/everybody_dance_now/"&gt;Website&lt;/a&gt;/&lt;a href="https://www.youtube.com/watch?v=PCBTZh41Ris"&gt;video&lt;/a&gt; from UC Berkeley researchers, 2018&lt;/p&gt;
&lt;p&gt;Think “Auto-Tune for dancing.” Using pose estimation and generative adversarial training, the researchers were able to make a fake video of any real person (the “target” person) dancing with great dance skills. The required input was only:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a short video of someone with great dance skills dancing&lt;/li&gt;
&lt;li&gt;a few minutes of video of the target person dancing (typically poorly since most people suck at dancing)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also saw Jensen Huang, the CEO of NVIDIA, show a video (made with this technique) of himself dancing like Michael Jackson. I’m glad I attended the GPU Tech Conference, haha.&lt;/p&gt;
&lt;h1&gt;Reinforcement learning&lt;/h1&gt;
&lt;h2&gt;World models — AI learning inside its own dream&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://worldmodels.github.io/"&gt;Website by Google Brain, 2018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/0*t6x7EEUGlErd8nLb.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Humans do not actually know or think about all the details of the world we live in. We behave based on the abstraction of the world that is in our heads. For example, if I ride on a bike, I don’t think of the gears/nuts/bolts of the bike; I just have a rough sense of where the wheels, seat, and handle are and how to interact with them. Why not use a similar approach for AI?&lt;/p&gt;
&lt;p&gt;This “world models” approach (again, created by David Ha et al) allows the “agent” (e.g. an AI that controls a car in a racing game) to create a generative model of the world/environment around it which is a simplification/abstraction of the actual environment. So, you can think of the world model as a dream that lives in the head of the AI. Then the AI can train via reinforcement learning in this “dream” to achieve better performance. So this approach is actually combining generative ML with reinforcement learning. By doing this, the researchers were able to achieve state-of-the-art performance on certain video game-playing tasks.&lt;/p&gt;
&lt;p&gt;[Update 2019/2/15] Building upon the above “world models” approach, Google just revealed &lt;a href="http://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html"&gt;PlaNet: Deep Planning Network for Reinforcement Learning&lt;/a&gt;, which achieved 5000% better data efficiency than previous approaches.&lt;/p&gt;
&lt;h2&gt;AlphaStar — Starcraft II AI that beats the top pro players&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"&gt;Blog post&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=cUTMhmVh1qs"&gt;e-sports-ish video&lt;/a&gt; by DeepMind (Google), 2019&lt;/p&gt;
&lt;p&gt;We’ve come a long way from the &lt;a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"&gt;historic Go matches between Lee Sedol and DeepMind’s AlphaGo&lt;/a&gt; that rocked the world, which was a mere 3 years ago in 2016 (check out the &lt;a href="https://www.netflix.com/jp-en/title/80190844"&gt;NetFlix documentary&lt;/a&gt;, which made some people cry). Then, it was even more amazing that AlphaZero in 2017 became better than AlphaGo at Go (and better than any other algorithm at chess, shogi AKA Japanese chess, etc.) despite not using any training data from human matches. But AlphaStar in 2019 is even &lt;strong&gt;more&lt;/strong&gt; amazing.&lt;/p&gt;
&lt;p&gt;Being a StarCraft fan myself since 1998, I can appreciate how the “…need to balance short and long-term goals and adapt to unexpected situations… poses a huge challenge.” It’s truly a difficult and complex game which requires understanding at multiple levels to play well. Research on Starcraft-playing algorithms have been ongoing since 2009.&lt;/p&gt;
&lt;p&gt;AlphaStar essentially used a combination of supervised learning (from human matches) and reinforcement learning (playing against itself) to achieve its results.&lt;/p&gt;
&lt;h1&gt;Humans training robots&lt;/h1&gt;
&lt;h2&gt;Teaching tasks to machines with a single human demonstration&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://news.developer.nvidia.com/new-ai-technique-helps-robots-work-alongside-humans/"&gt;Article&lt;/a&gt;/&lt;a href="https://www.youtube.com/watch?time_continue=1&amp;amp;v=B7ZT5oSnRys"&gt;video&lt;/a&gt; by NVIDIA researchers, 2018&lt;/p&gt;
&lt;p&gt;I can think of 3 typical approaches to teaching robots to do something, but all take a lot of time/labor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manually program the robot’s joint rotations etc. for each situation&lt;/li&gt;
&lt;li&gt;Let the robot try the task many, many times (reinforcement learning)&lt;/li&gt;
&lt;li&gt;Demonstrate a task to the robot many, many times&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Typically, one major criticism of deep learning is that it’s very costly to produce the millions of examples (data) that make the computer perform well. But increasingly, there are ways to not rely on such costly data.&lt;/p&gt;
&lt;p&gt;The researchers figured out a way for a robot arm to successfully perform a task (such as “pick up the blocks and stack them so that they are in the order: red block, blue block, orange block”) based on a &lt;strong&gt;single&lt;/strong&gt; video of a &lt;strong&gt;single&lt;/strong&gt; human demonstration (a physical real human hand moving the blocks), even if the video was shot from a different angle. The algorithm actually generates a human-readable description of the task it plans to do, which is great for troubleshooting. The algorithm relies on object detection with pose estimation, synthetic training data generation, and simulation-to-reality transfer.&lt;/p&gt;
&lt;h1&gt;Unsupervised machine translation&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://code.fb.com/ai-research/unsupervised-machine-translation-a-novel-approach-to-provide-fast-accurate-translations-for-more-languages/"&gt;Blog post by Facebook AI Research, 2018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Typically, you would need a huge training dataset of translated documents (e.g. professional translations of United Nations proceedings) to do machine translation well (i.e. &lt;strong&gt;supervised&lt;/strong&gt; learning). Of course, many topics and language pairs don’t have high-quality, plentiful training data. In this paper, researchers showed that it’s possible to use &lt;strong&gt;unsupervised&lt;/strong&gt; learning (i.e. using no translation data and just using unrelated corpuses of text in each language), it’s possible to reach the translation quality of state-of-the-art &lt;strong&gt;supervised&lt;/strong&gt; learning approaches. Wow.&lt;/p&gt;
&lt;p&gt;The basic idea is that, in any language, certain words/concepts will tend to appear in close proximity (e.g. “furry” and “cat”). They describe this as “embeddings of words in different languages share similar neighborhood structure.” I mean, OK, I get the idea, but it’s still mind-blowing that using this approach they can reach such high translation quality without training on translation datasets.&lt;/p&gt;
&lt;h1&gt;Closing&lt;/h1&gt;
&lt;p&gt;I hope this post made you more excited about developments in ML/AI, if you weren’t already. Maybe I’ll write another similar post in a year from now. Please feel free to leave any thoughts/comments here or e-mail me at jerrychi123 [at] gmail.com.&lt;/p&gt;
&lt;p&gt;What a time to be alive! =D&lt;/p&gt;</content><category term="machine learning"></category><category term="machine learning"></category><category term="research"></category><category term="computer vision"></category><category term="unsupervised learning"></category></entry><entry><title>Thoughts On My Past 2018 And Future 2019</title><link href="https://jerrychi.com/thoughts-on-my-past-2018-and-future-2019.html" rel="alternate"></link><published>2019-01-02T00:00:00+09:00</published><updated>2019-01-02T00:00:00+09:00</updated><author><name>Jerry Chi</name></author><id>tag:jerrychi.com,2019-01-02:/thoughts-on-my-past-2018-and-future-2019.html</id><summary type="html">&lt;p&gt;4 min read&lt;/p&gt;
&lt;h1&gt;Thoughts on my past (2018) and future (2019~)&lt;/h1&gt;
&lt;p&gt;I finally decided to start posting on Medium, sure took me long enough. My &lt;a href="http://peacej2.blogspot.com/"&gt;past blog on Blogger&lt;/a&gt; is still there (only 5 posts in 10 years.. hmm I guess I’m not very prolific).&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*aG461LzFzge8h3JSP1laXQ.jpeg"&gt; 
&lt;em&gt;Randomly decided to apply …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;4 min read&lt;/p&gt;
&lt;h1&gt;Thoughts on my past (2018) and future (2019~)&lt;/h1&gt;
&lt;p&gt;I finally decided to start posting on Medium, sure took me long enough. My &lt;a href="http://peacej2.blogspot.com/"&gt;past blog on Blogger&lt;/a&gt; is still there (only 5 posts in 10 years.. hmm I guess I’m not very prolific).&lt;/p&gt;
&lt;p&gt;&lt;img alt="todo add alt text" src="https://miro.medium.com/max/20000/1*aG461LzFzge8h3JSP1laXQ.jpeg"&gt; 
&lt;em&gt;Randomly decided to apply style transfer (deep learning) to a photo of myself&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;My 2018&lt;/h1&gt;
&lt;p&gt;I have a bad habit of being generally dissatisfied with my accomplishments, but I should be thankful for the opportunities to do what I did in 2018:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Built the data science team at &lt;a href="https://www.businessinsider.com/smartnews-free-news-app-2018-9"&gt;SmartNews&lt;/a&gt; from scratch (still hiring in &lt;a href="https://smartnews.workable.com/j/711F993A53"&gt;Tokyo&lt;/a&gt; and &lt;a href="https://smartnews.workable.com/j/D93F58DBBD"&gt;San Francisco&lt;/a&gt;). We’re now 5 members doing analytics (and a bit of machine learning and data engineering) across the whole company: product, content, marketing, sales, etc. All the team members are talented, friendly, and multilingual, and I’m lucky to be leading such a team. As a company, we became the biggest news app in Japan and the 3rd biggest on Android in the US (if we use certain convenient definitions).&lt;/li&gt;
&lt;li&gt;Became a mentor for &lt;a href="https://developers.google.com/programs/launchpad/accelerators/"&gt;Google Launchpad Accelerator Tokyo&lt;/a&gt;, which is an accelerator program for “Japanese startups focused on using AI/ML with a global mindset.” It was really fun and fulfilling to advise &lt;a href="https://developers-jp.googleblog.com/2018/11/google-launchpad-accelerator-tokyo7.html"&gt;these innovative startups&lt;/a&gt; with huge potential, and I learned a lot as well from them and other mentors.&lt;/li&gt;
&lt;li&gt;Became an External Director (社外取締役) of &lt;a href="http://mtic.co.jp"&gt;MTIC&lt;/a&gt;, a company started by an old friend. The company helps blue collar workers (mostly from the Southeast Asia) find jobs in Japan. This is actually a huge issue for the Japanese government; in 2018, the government &lt;a href="https://www.reuters.com/article/us-japan-immigration/japan-aims-to-open-door-wider-to-blue-collar-workers-idUSKCN1N703I"&gt;made big progress in terms of planning to open up and support many more blue collar foreign workers&lt;/a&gt;, given the demographic / labor crisis the country is in. You can hear more &lt;a href="http://www.disruptingjapan.com/is-there-finally-a-practical-way-for-foreigners-to-live-in-japan/"&gt;described by the CEO in the Disrupting Japan podcast&lt;/a&gt;. In the long term, we’d like to use machine learning for job matching, but we’re not quite there yet…&lt;/li&gt;
&lt;li&gt;Helped prepare my wife (Shirei) for Google interviews.. and she got in! She now works on scalable user support (i.e. how to keep millions of users happy) especially for the Pixel 3 phone in Japan. She has better work-life balance now too, and she can see her sister (who also works at Google) more often. By the way the Pixel 3 is the best phone I’ve ever used; it uses &lt;a href="https://ai.googleblog.com/2018/11/learning-to-predict-depth-on-pixel-3.html"&gt;rad machine learning to take amazing photos&lt;/a&gt;. Especially the &lt;a href="https://gearpatrol.com/2018/12/06/google-pixel-3-night-sight-review/"&gt;Night Sight feature&lt;/a&gt; is way better than anything Apple/Samsung etc. has made.&lt;/li&gt;
&lt;li&gt;Attended 3 cool conferences (Google Cloud Next, Nvidia GPU Technology Conference, and &lt;a href="https://www.youtube.com/playlist?list=PLGVZCDnMOq0q1JoiGlc6Mit-lWcK--yRS"&gt;PyData LA&lt;/a&gt;), all for the first time. All 3 involved lots of talk about machine learning and how it’s becoming increasingly easy to be used practically by more people.&lt;/li&gt;
&lt;li&gt;Attended various fun machine learning events in Tokyo (&lt;a href="https://www.meetup.com/Machine-Learning-Meetup-by-team-ai/"&gt;Team AI&lt;/a&gt;, &lt;a href="https://www.meetup.com/Machine-Learning-Tokyo/"&gt;Machine Learning Tokyo&lt;/a&gt;, &lt;a href="https://mlct.connpass.com/"&gt;ML Casual Talks @ Mercari&lt;/a&gt;). One awesome event was the &lt;a href="https://machinelearningtokyo.com/2018/06/03/machine-learning-tokyo-gans/"&gt;hands-on workshop on GANs (generative adversarial networks)&lt;/a&gt;. Sometimes I can’t believe how much great education I can get for free. Big thanks to the organizers!&lt;/li&gt;
&lt;li&gt;Had lunch with David Ha, the head of Google Brain Tokyo. This guy is basically my hero. He went from being a managing director at Goldman to being a prolific machine learning researcher (which kind of started from a &lt;a href="http://blog.otoro.net/2015/12/28/recurrent-net-dreams-up-fake-chinese-characters-in-vector-format-with-tensorflow/"&gt;side project&lt;/a&gt; he had) and &lt;a href="https://arxiv.org/abs/1803.10122"&gt;his work is really creative and amazing and sometimes mind-blowing&lt;/a&gt;. Thanks David for the advice~&lt;/li&gt;
&lt;li&gt;Wrote this Medium post in which every single bullet point mentioned machine learning (are you getting sick of it yet? yeah? well too bad! =P )&lt;/li&gt;
&lt;li&gt;Completed Zelda Breath of the Wild without any use of machine learning. (well, that’s not exactly true.. I relied on Zelda-related Google searches and recommended Youtube videos etc. which are powered by ML.) Really great game. Thanks to my wife Shirei for buying the Nintendo Switch and Zelda for me ❤&lt;/li&gt;
&lt;li&gt;Travelled to Kanazawa 金沢, Kawaguchi-ko 河口湖, Seoul, and Taipei (for weddings / vacation) and San Francisco and LA (for work / conference). In SF, I did fun office tours of Airbnb, Slack, Twitter, and Mercari. (thank you Naoya, Shiho, and Fabien!)&lt;/li&gt;
&lt;li&gt;Interesting books I read: &lt;a href="https://www.amazon.com/dp/B07DRPGGQ7/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1"&gt;High Growth Handbook&lt;/a&gt; (for mid/late-stage startups) and &lt;a href="https://www.amazon.com/gp/product/B0722H41SG/"&gt;Developer Hegemony: the Future of Labor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Good entertainment: Westworld season 2; Attack on Titan 進撃の巨人 season 3; &lt;a href="https://www.netflix.com/title/80174974"&gt;Devilman Crybaby&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;2019 Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Read more books. I didn’t read enough books in 2018. My time spent on news articles / blog posts was maybe 10x that of books..probably not the best ratio.&lt;/li&gt;
&lt;li&gt;Study and apply more machine learning. I actually didn’t make as much progress as I wanted to in 2018 in terms of completing online courses, reading papers, etc. although I did still learn a fair amount at work and otherwise. In 2017, I was mostly in Finland, so I had many dark, cold, lonely nights and weekends to myself to focus on studying; in 2018 I had too many distractions. The field is progressing quickly, and I need to step it up in 2019.&lt;/li&gt;
&lt;li&gt;[typical new year resolution about exercising more / becoming more healthy]&lt;/li&gt;
&lt;li&gt;One more goal.. this is a secret for now&lt;/li&gt;
&lt;/ul&gt;</content><category term="diary"></category><category term="goals"></category><category term="self-improvement"></category></entry></feed>